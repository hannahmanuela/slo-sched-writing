%-------------------------------------------------------------------------------
\section{Evaluation}
\label{s:eval}
%-------------------------------------------------------------------------------

We begin by evaluating the existing alternatives to \cgroups{} that currently
exist in Linux in \autoref{ss:existing}. We then explore the performance of our
edit, in relation to the following questions:
\begin{enumerate}
    \item Does the new \schedbe{} isolate LC from BE workloads for real
    applications?
    \item bloop bleep something about starvation?
    \item How much does the patch cost?
\end{enumerate}

All the graphs in this paper run on Linux version 6.14.2, the baseline version
that our patch builds on.

\subsection{Existing approaches}\label{ss:existing}

To show the benefits of \schedbe{}, we compare with existing alternatives within
Linux. The first is real time scheduling, because this is a place where Linux
already enforces categorical global priorities: processes that run in real time
have strict priority over other processes. The second is the existing mechansism
of \schedidle{}, which \schedbe{} is based on.

\subsubsection{Realtime scheduling}

Linux enforces the categorical separation of real time tasks by using
\textit{scheduling classes}. Each scheduling class exists completely separately:
classes maintain their own runqueues and per-entity state; implement their own
scheduling algorithms to choose from the entities on their runqueue; and balance
the load across runqueues on different cores.

Linux isolates strictly between different scheduling classes: it only schedules
a lower scheduling class if the higher scheduling classes found nothing to run,
and each scheduling class tries to steal work from other cores before returning
that it has nothing to run --- these two checks represent the entry and exit
synchronization points. It is thereby true that if something in the Normal
scheduling class is running, it means there are no Fifo tasks waiting to run
anywhere on the machine.

This points to a possible alternative: run LC in the Fifo scheduling class and BE
in Normal.\footnote{The Deadline scheduling class is not a good fit, since it
requires accurate knowledge of a processes runtime (processing time per request)
and period (when requests come in)} Fifo runs a priority scheduler: it has 99
priorities, each takes strict precedent over the one lower; within priorities
the scheduler enforces a global first-in-first-out (hence the Fifo class name),
based on when processes become runnable.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/unedited-rt-low-two.png}
        \caption{Low load}\label{fig:unedited-rt-low-two}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/unedited-rt-high-two.png}
        \caption{High load}\label{fig:unedited-rt-high-two}
    \end{subfigure}
    \vspace{4pt}
    \caption{Results of the same experiment, with LC running as a real time process}\label{fig:unedited-rt}
\end{figure}

We run the same experiment as in \autoref{s:intro}, but put the LC task in the
Fifo scheduling class, and leave BE tasks with the default process weight.
\autoref{fig:unedited-rt} shows the resulting measured latencies in the same low
and high load setting as previously. As expected, we see that Linux is able to
isolate the two very well. 

However, this is an untenable solution because of Fifo's run-to-completion
scheduling, which is known to have a failure mode of head-of-line (HoL) blocking
under varied request processing times, where long-running requests monopolize
the CPU while short requests wait in the queue. The Fifo scheduler also enforces
not only cross-core isolation between different priorities, but also a global
ordering within the same priority.

The takeaway is that Linux's current mechanism of scheduling classes can isolate
workloads effectively, but existing scheduling classes use scheduling
algorithms that are not a good fit for modern workloads.

\subsubsection{\schedidle}\label{ss:schedidle}

\schedidle{} is a scheduling \textit{policy}. Policies are not full scheduling
classes, but allow for special casing within a scheduling class. The Normal
scheduling class supports two policies: \schednormal{}, the default, and
\schedidle{}. Because both policies are in the same class the scheduler for the
Normal class is in charge of both: it keeps the tasks of both policies on the
same runqueue, and they are all scheduled using the same algorithm. 

As we discussed in \autoref{s:maybe-solution}, \schedidle{} is special cased,
but only on wakeup. In doing so, Linux created a half scheduling class.
\schedbe{} effectively turns \schedidle{} into a scheduling class.\hmng{this
feels like too good of a nugget to be hidden in the eval?}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/unedited-idle-low-two.png}
        \caption{Low load}\label{fig:unedited-idle-low-two}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/unedited-idle-high-two.png}
        \caption{High load}\label{fig:unedited-idle-high-two}
    \end{subfigure}
    \vspace{4pt}
    \caption{using \schedidle{}}\label{fig:unedited-idle}
\end{figure}

And indeed, we find that when we use cgroups' new cpu.idle interface feature,
the latency impact of the BE tasks decreases, although it does not entirely drop
to what we saw with the Fifo class.\ \autoref{fig:unedited-idle} shows the
results, for the familiar settings of low and high load. The jump we see in the
mean latency has decreased from peaks as high as 13ms to around 7ms (even though
in principle they now have a higher weight).

\subsection{\schedbe}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/patched-idle-low-two.png}
        \caption{Low load}\label{fig:patched-idle-low-two}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/patched-idle-high-two.png}
        \caption{High load}\label{fig:patched-idle-high-two}
    \end{subfigure}
    \vspace{4pt}
    \caption{using a patched \schedidle{} that steals queued \schednormal{}
    tasks before running \schedidle{} ones}\label{fig:patched-idle}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-patched.png}
    \caption{The BE threads are colored in two different shades of green, the LC
    threads are the grey ones, the red vertical lines are the scheduler
    initially choosing a BE thread, which leads to an attempt to steal a queued
    LC one. As a result, BE threads only run when there are no queued LC
    threads.}\label{fig:schedviz-patched}
\end{figure}

We finally run the same exoeriment, again using the \cgroups{} interface to set
the BE workloads to be marked as idle via the \cgroups{} api, except that now
this puts the BE tasks in \schedbe{}. We can see the resulting performance in
\autoref{fig:patched-idle}. As desired the latency of the server remains stable
after the background tasks start. This does not mean that the background task
never runs: the lower graph still shows iterations of image resizing being done.
The difference is that now the background tasks will reliably get interrupted
when the LC server has a request to process. We can see this happening in an
outtake of the schedviz visualization for one of the runs in
\autoref{fig:schedviz-patched}. The green BE processes run only in the gaps
where there is no queued LC process, and are immediately preempted when one
wakes up, on whatever core that may be. The red lines show when the core has
chosen intially to run a BE process, sometimes followed by an LC process that
was stolen as a result running next, sometimes by the BE process actually
running because there was no queued LC thread to steal.


\subsubsection{Realistic applications}

This work focuses on cpu allocation, and thus will show the strongest gains for
applications that are cpu bound. 

We test the patch in two different settings: the completely cpu-bound example
server we have been using for the graphs in this paper so far, and an example
web application that we run using Kubernetes, that uses a mix of i/o and cpu.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-unedited.png}
    \caption{E2E repsonse times for the social application, before and after
    starting a request on a BE service}\label{fig:kubernetes-unedited}
\end{figure}

For the web application, we build a REST-api on top of FastAPI, and deploy it
using uvicorn workers. The web application itself is a social application, it
has a connection to an in-memory sqlite database, which is makes requests to and
processes data from. \autoref{fig:kubernetes-unedited} shows the results.

\subsection{Occurrence and effects of starvation}

We begin by looking at how Linux currently avoids starvation, and the effects
this has, and then explore the downsides of starvation and how/when they occur.

\subsection{Linux's starvation avoidance}

Linux in its current form safeguards heavily against starvation. As we saw
earlier, real time Fifo applications have strict priority and thus represent a
possible antagonist that would be able to starve everything else. However, Linux
has two different safe guards that protect against this. One is that Fifo is as
a scheduling class rate-limited: there are tuneable parameters at
\texttt{/proc/sys/kernel/sched\_rt\_runtime\_us} and
\texttt{/proc/sys/kernel/sched\_rt\_period\_us}, that together define a rate limit
for the Fifo class as a whole. The failsafes go further than just that though:
even when set to be equal (ie Fifo gets the full runtime each period if it
wants), the Normal scheduling class also has a so-called \textit{deadline
server}, which is basically a process is in the Deadline scheduling class with a
small amount of runtime per period~\cite{TODO}.
% https://lore.kernel.org/lkml/20200807095051.385985-5-juri.lelli@redhat.com/ 


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/rt-throttled.png}
    \caption{LC in real time, throttling}\label{fig:realtime-throttled}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/patched-not-throttled.png}
        \caption{BE in \schedbe{}, no throttling}\label{fig:patched-not-throttled}
    \end{subfigure}
    \vspace{4pt}
    \caption{throttling the LC server under extremely high utilization, vs
    letting it starve the BE userspace processes}
\end{figure}


We can see this in action if we turn up the utilization on the real time
experiment: if we run it so that the baseline utilization is oversaturated
(100\%), we see spikes begin to appear when the Fifo server gets throttled in
favor of running the bg tasks, and parallel spikes where the background tasks
are allowed to run. This is shown in \autoref{fig:realtime-throttled}. We can
see the spikes in latency for the server and throughput for the background task
as the throttling kicks in. Notice also the increase of the slope of response
times after starting the background tasks: the deadline server mechanism only
kicks in if there is load to run, so once it does the resulting throttling
degrades the servers performance signigicantly. If this were taking place during
a load spike while waiting for new server instances to start, the presence of BE
load would significantly impact the queue length/performance degradation while
the new server starts up.

On the other hand, we see in \autoref{fig:patched-not-throttled} what happens
if, in that same experiment, you allow the BE userspace process to be starved.
Notice that the BE does not make progress until the very end, when the server is
done processing the requests (the experiment stops the client after 30 seconds
and the server from then finishes the backlog of requests).


\subsection{Effects of choosing to starve}

We now explore what happens if you do let processes starve, and do this by
looking at two examples of undesirable behavior: (1) what happens to TCP
connections of starved processes, and (2) what happens to timer-based code.


\hmng{Also are going to need to say something about control mechansisms to stop
eg an accidental infinite loop needing to be higher priority anyway}



% \subsection{Verifying patch behavior}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.49\columnwidth}
%         \includegraphics[width=\columnwidth]{graphs/schedviz-steal-unedited.png}
%         \caption{unedited Linux}
%     \end{subfigure}
%     \hspace{\fill}
%     \begin{subfigure}[t]{0.49\columnwidth}
%         \includegraphics[width=\columnwidth]{graphs/schedviz-steal-patched.png}
%         \caption{patched Linux}
%     \end{subfigure}
%     \vspace{4pt}
%     \caption{microbench on two cores; in both diagrams the different shades of blue are the
%     \schednormal{} processes and the yellow is the \schedidle{} process}\label{fig:steal-micro}
% \end{figure}

% We begin by looking at the behavior of the patch in a microbenchmark. On three
% cores, we run two \schednormal{} processes and one \schedidle{} process. We
% visualize the scheduling decisions made by both an unedited and patched version
% of the kernel in \autoref{fig:steal-micro}. We observe that the scheduler
% initially runs two of the \schednormal{} processes on one core and the third on
% the other core. When the latter finishes, the unedited version of Linux will run
% the \schedidle{} process on that core; whereas the patched version will steal
% the queued \schednormal{} process from the other core. 

% We conclude that the patch works as expected, and is able to enforce the
% categorical separation of \schednormal{}, ie LC, and \schedidle{}, ie BE,
% workloads across cores.