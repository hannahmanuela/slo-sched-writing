%-------------------------------------------------------------------------------
\section{Evaluation}
\label{s:eval}
%-------------------------------------------------------------------------------

This section answers the following questions:
\begin{enumerate}
    \item Does the new \schedbe{} isolate LC from BE workloads?
    \item Can parked processes resume correctly?
    \item How much does \schedbe{} cost?
\end{enumerate}

All the graphs in this paper run on Linux version 6.14.2, the baseline version
that our patch builds on. \hmng{TODO put in specs of ..? zg machines? I didn't
always use all of the cores, but ig I can say that on a per-experiment basis}

\subsection{Does \schedbe{} isolate?}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-schedbe-low.png}
        \caption{Low load}\label{fig:srv-bg-schedbe-low}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-schedbe-high.png}
        \caption{High load}\label{fig:srv-bg-schedbe-high}
    \end{subfigure}
    \vspace{4pt}
    \caption{same expiriment as in \autoref{fig:srv-bg-unedited}, but with the
    BEs running in \schedbe{}}\label{fig:srv-bg-schedbe}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-schedbe.png}
    \caption{The BE threads are colored in two different shades of green, the LC
    threads are the grey ones, the red vertical lines are the scheduler
    initially choosing a BE thread, which leads to an attempt to steal a queued
    LC one. As a result, BE threads only run when there are no queued LC
    threads.}\label{fig:schedviz-schedbe}
\end{figure}

We run the microbenchmark experiment from \autoref{fig:srv-bg-unedited} using
\schedbe{}. We can see the resulting performance in
\autoref{fig:srv-bg-schedbe}. As desired, the latency of the server remains
stable after the background tasks start. This does not mean that the background
task never runs: the lower graph still shows iterations of image resizing being
done. The difference is that now the background tasks will reliably get
interrupted when the LC server has a request to process. 

\autoref{fig:schedviz-schedbe} shows the desired behavior happening in an
outtake of a schedviz visualization. The green BE processes run only in the gaps
where there is no queued LC process, and are immediately preempted when one
wakes up, on whatever core that may be. The vertical red lines show when the
core has chosen intially to run a BE process. As we can see, this is sometimes
followed by just running the BE, but often by the core running an LC process,
meaning that it succesfully found and stole a queued and waiting LC thread.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-schedbe.png}
    \caption{The same experiment as in \autoref{fig:kubernetes-unedited}, but
    running the BE as a \schedbe{} task}\label{fig:kubernetes-schedbe}
\end{figure}

We also run the Kubernetes application from \autoref{fig:kubernetes-unedited}
using \schedbe{}. The results are in \autoref{fig:kubernetes-schedbe}. We can
see that, just as in the microbenchmark, the latency profile of the web
application looks largely the same before and after starting the image resize
job. It is not entirely without spikes, but the spikes come from interference
with Python runtimes and Kubernetes controllers, and are the same before and
after starting the BE tasks. Importantly, the baseline median latency of the LC
web application stays stable after starting the the BE image resizing. 

\subsection{Can parked processes resume correctly?}\label{ss:eval:parking}


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/parked-kubernetes.png}
    \caption{A \beclass{} process in a Kubernetes pod, parked while the
    antagonist runs and then resuming}\label{fig:parked-kubernetes}
\end{figure}

After being parked for multiple minutes in the middle of processing a request, a
BE image resize job running in Kubernetes is able to resume and return the final
result to the client. \autoref{fig:parked-kubernetes} shows the throughput over
time of the image resize job. A couple seconds after sending a request to the
image resize pod, we start an entirely cpu-bound antagonist on the same set of
cores as the BE is running. We can see that the image resize job is completely
paused for about two and a half minutes, until the antagonist is done running.
The job then continues and completes. The client connection did not experience
any issues, even when setting the TCP keepalive timeout to be less than a
minute.



\subsection{Cost of \schedbe{}}

We evaluate the cost of the additional checks required by \schedbe{} by looking
at how long the code takes to run, and how much lock contention it
creates.\hmng{This is a pretty big todo}




\subsection{Existing approaches}\label{ss:eval:existing}

To show the benefits of \schedbe{}, we compare with existing alternatives to
\cgroups{} within Linux.

\subsubsection{Realtime scheduling}

As we discussed in \autoref{s:design}, Linux enforces the categorical
separation of tasks in different scheduling classes. 

This points to a possible alternative to using \cgroups{}: run LC in the
\fifoclass{} scheduling class and BE in \normalclass{}.\footnote{The
\deadlineclass{} scheduling class is not a good fit, since it requires accurate
knowledge of a processes runtime (processing time per request) and period (when
requests come in)} \fifoclass{} runs a priority scheduler: it has 99 priorities,
each takes strict precedent over the one lower; within priorities the scheduler
enforces a global first-in-first-out (hence the \fifoclass{} class name), based
on when processes become runnable.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-rt-low.png}
        \caption{Low load}\label{fig:srv-bg-rt-low}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-rt-high.png}
        \caption{High load}\label{fig:srv-bg-rt-high}
    \end{subfigure}
    \vspace{4pt}
    \caption{Results of the same experiment as in \autoref{fig:srv-bg-unedited}
    and \autoref{fig:srv-bg-schedbe}, with LC running as a real time
    process}\label{fig:srv-bg-rt}
\end{figure}

We run the same microbenchmark experiment, but put the LC task in the
\fifoclass{} scheduling class, and leave BE tasks with the default
\normalclass{} scheduler and weight. \autoref{fig:srv-bg-rt} shows the resulting
measured latencies in the same low and high load setting as previously. As
expected, we see that Linux is able to isolate the two very well.

However, this is an untenable solution because of \fifoclass{}'s
run-to-completion scheduling, which is known to have a failure mode of
head-of-line (HoL) blocking under varied request processing times, where
long-running requests monopolize the CPU while short requests wait in the queue.
The \fifoclass{} scheduler also enforces not only cross-core isolation between
different priorities, but also a global ordering within the same priority. This
requires more synchronization, and is a stricter policy than we need.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/overload-rt.png}
        \caption{LC in real time, throttling}\label{fig:overload-rt}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/overload-schedbe.png}
        \caption{BE in \schedbe{}, no throttling}\label{fig:overload-schedbe}
    \end{subfigure}
    \vspace{4pt}
    \caption{throttling the LC server under extremely high utilization, versus
    letting it park the BE userspace processes}
\end{figure}

In addition, under high utilization of real time processes, Linux does not park
BEs or do something equivalent, but instead throttles the \fifoclass{} LC
processes~\cite{lkml-deadline-srv}. We can see this happening when we run the
same microbenchmark experiment from \autoref{fig:srv-bg-unedited}, with the LC
in \fifoclass{} and the BE in \normalclass{}, and a much higher baseline
utilization ($\sim$ 100\%). The results are in \autoref{fig:overload-rt}. We see
spikes begin to appear after starting the BE task, as the \fifoclass{} server
gets throttled in favor of running the BE tasks; we see parallel spikes in the
BE's throughpu in the bottom graph. Notice also the increase of the slope of
response times after starting the background tasks: the deadline server
mechanism only kicks in if there is load to run, so once it does the resulting
throttling degrades the servers performance signigicantly.

By comparison, \autoref{fig:overload-schedbe} shows what happens if, in that
same experiment, \schedbe{} allows the BE userspace process to be parked. Notice
that the BE does not make progress until the very end, when the server is done
processing the requests (the experiment stops the client after 30 seconds and
the server from then finishes the backlog of requests).

The takeaway is that Linux's current mechanism of scheduling classes can isolate
workloads effectively, but existing scheduling classes use scheduling
algorithms that are not a good fit for modern workloads.

\subsubsection{\schedidle}\label{ss:schedidle}

As we discussed in \autoref{s:implementation}, \schedidle{} implements some but
not all of the isolation required of \beclass{}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-idle-low.png}
        \caption{Low load}\label{fig:srv-bg-idle-low}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-idle-high.png}
        \caption{High load}\label{fig:srv-bg-idle-high}
    \end{subfigure}
    \vspace{4pt}
    \caption{using \schedidle{}}\label{fig:srv-bg-idle}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-idle.png}
    \caption{The same experiment as in \autoref{fig:kubernetes-unedited}, but
    running the BE as a \schedidle{} task}\label{fig:kubernetes-idle}
\end{figure}

\autoref{fig:srv-bg-idle} shows the result of running the BE jobs in the
microbenchmark from \autoref{fig:srv-bg-unedited} in \schedidle{}. Average
latency still increases from $\sim$6ms to $\sim$8ms. Although this increase is
smaller than the original increase to $\sim$15ms using the standard weight
interface, it is still high compared to the 0ms increase that \schedbe{}
achieves.

\autoref{fig:kubernetes-idle} shows similar results for the Kubernetes
experiment from \autoref{fig:kubernetes-unedited}.