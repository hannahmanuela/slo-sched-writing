\section{Design of \sys}
\label{sec:global}

\sys is a new design for scheduling processes that enforces group
weights globally across cores.  It assigns each runnable process a
global virtual time based on its group weight. \sys maintains a heap
of these processes based on their virtual runtimes, with the process
with lowest virtual time at the front of the heap
(\autoref{sec:vt}). To implement this heap in a scalable manner, \sys
uses a {\it relaxed} heap based on concurrent priority
queues~\cite{multiqueue:2015},
which internally uses many heaps; when selecting a process to
schedule, it samples two heaps and chooses the process with the
smallest virtual time.  It is relaxed in the sense that the chosen
process may have a higher virtual time than the global virtual time,
but the random sampling ensures that the chosen process has a virtual
time that is close to global the minimum.  \sys leverages
the existences of multiple heaps to allow a core to select a process
that it has run recently so that the process runs with a warm L1/L2
cache (\autoref{sec:relaxed}). 

\subsection{Global virtual time}
\label{sec:vt}

\begin{figure}
\input{code/group}
\caption{The state for each group}
\label{fig:group}
\end{figure}

\autoref{fig:group} shows the state that \sys maintains for each
cgroup: \cc{weight}, the groups weight, \cc{vruntime}, which is the
next available virtual runtime for a process in the cgroup to run, and
\cc{min_vt_deq}, which is the virtual runtime when a group goes to
  ``sleep'' (i.e., when all group's processes are sleeping).

\begin{figure}
\input{code/enq}
\caption{Enqueue a process}
\label{fig:enq}
\end{figure}

When a process of in a group becomes runnable, \sys assigns it a
virtual time and inserts it in the global heap, as show in
\autoref{fig:enq}. Consider a \cc{tick_length} of 1000us and two
groups, $g_1$ with weight 10 and $g_2$ with weight 20. When a process
of $g_1$ becomes runnable it will get assigned the current group's
virtual time and increases it by 100 ($= 1000/10$).  If concurrently,
on another core, another process of $g_1$ becomes runnable it will be
assigned a virtual time that is 100 larger than the first process, and
$g_1$'s virtual time will be incremented by 100, ensuring the
processes are correctly spaced in virtual time by 100.

When a process of $g_2$ becomes runnable, it will increases its
group's virtual time by 50 ($= 1000/20$), and the processes of $g_2$
will be spaced in virtual time by 50.  If we look over a window of a
1000 in virtual time, $g_1$ will run 10 times and $g_2$ will run 20
times, reflecting their relative weights of 10 and 20.

\begin{figure}
\input{code/adjust}
\caption{Adjust virtual runtime when a process yield before a full clock tick.}
\label{fig:adjust}
\end{figure}

If a process yields its core before \cc{tick_length}, \sys adjusts the
group's virtual time to reflect that the process didn't use the full
tick, as shown in \autoref{fig:adjust}.  The adjustment moves move the
group's virtual time backwards so that the next process of the group
inherits the time that the first process didn't use.

\begin{figure}
\input{code/lag}
\caption{Set group's virtual time when it becomes runnable.}
\label{fig:lag}
\end{figure}

When the last running process of a group goes to sleep, \sys remembers
the minimum virtual time of the heap in the process group's
\cc{min_vt_deq}. It does so to set the group's \cc{vruntime} when
the group becomes runnable again, as shown in \autoref{fig:lag}.  When
the group becomes runnable, the global virtual runtime may have moved
ahead since the group went to sleep. To account for this, \sys sets
the group's virtual time to the current minimum global virtual time,
adjusted for how much the group was ahead or behind the global minimum
when the group dequeued.

The way \sys computes virtual time is similar to Linux's fair-share
scheduler with the main difference being that \sys must compute the
virtual time globally, and, handle multiple cores concurrently
assigning virtual times to processes of the same group.  In Linux
virtual time is per run queue (and not global), and updated only by
one core. As a result, Linux, for example, doesn't need to update the
group's virtual on enqueue, doesn't need to adjust it if a process
runs shorter than its full tick, and doesn't need to set a group's
virtual time when it wakes up.

\subsection{\sys's relaxed heap}
\label{sec:relaxed}

\sys's heap of runnable processes is often written by many cores
concurrently: cores remove the process with the minimal virtual time
from the heap and insert runnable processes in the heap based on their
virtual times.  To avoid operations contending on the global heap,
\sys uses a multi-heap design with a relaxed global minimum, which is
based on relaxed concurrent priority queues~\cite{multiqueue:2015,
  williams:arxiv, williams:multiqueue}.

In concurrent priority queues, a core inserts an element in a random
queue out of $n \times d$ queues, where $n$ is the number of cores and
$d$ is a small constant (typically 2).  To find the minimum element to delete,
concurrent priority queues chooses two random queues and removes the
smallest one. The intuition behind this idea is from randomized load
balancing: choosing two randomly-chosen machines and add a ball to the
least-loaded one results in a maximum load close to the average
load with high
probability~\cite{mitzenmacher:power,berenbrink:balanced}.

Concurrent priority queues are attractive for implementing \sys
because they allow for concurrent write operations.  Each queue has
its own lock and cores retry choosing a random queue if the chosen
queue is locked by another core.  Williams et AL. show---in theory and
in practice---that concurrent priority queues can achieve throughout
that scales with increasing number of cores and that the rank error (the
distance of the deleted element to the best element) is in expectation
$O(c \log c)$ with high probability~\cite{williams:multiqueue}.

Concurrent priority queues don't provide core-affinity for processes.
Because all processes are totally ordered by virtual time and cores
randomly sample queues, it is likely that if one core has run a
process, that process will run next on a different core. This results
in poor L1/L2 cache performance for all processes.  \sys modifies the
selection procedure to realize some core affinity while preserving
global weights.

Specifically, \sys modifies the select operation of concurrent
priority queues to allow cores to choose a process that it has run
recently so that the process can benefit from a warm L1/L2 cache.  A
core remembers the last process it ran, which, if it is runnable, is
in some random heap.  If the process is at the front of that heap,
\sys samples one random heap.  If the process in the sampled heap
belongs to the same cgroup (i.e., has the same weight) as the
recently-run process, \sys removes the recently-run process and runs
it.  This process may have a higher virtual time than the non-selected
process, but since that process is at the front of a heap, another
core is likely to select it soon.

\begin{figure}
\input{code/schedule}
\caption{Selecting with affinity}
\label{fig:schedule}
\end{figure}

\autoref{fig:schedule} shows core-affinity selection in more
detail. If \cc{cp}, the process that the core has run recently, has
already been selected by another core, \cc{min_affinity} returns
immediately and falls back to sampling two heaps without consideration
of affinity, as described above.

Next, \cc{min_affinity} checks if \cc{cp} is at the front of the
random heap that it was inserted in. If so, it acquires the lock of
that heap and re-checks if \cc{cp} is still at the front, in case
another core slipped between the first check and the lock acquisition.
Next, \cc{min_affinity} chooses a random heap and invokes
\cc{select_affinity}, which checks if the min process of the
randomly-chosen heap belongs to the same cgroup or has a higher
virtual time. If so, the \cc{min_affinity} ignores the randomly-chosen
heap and returns \cc{cp}. The process \cc{cp} will now run on the same
core as it ran recently on and with a warm L1/L2 cache.

If \cc{select_affinity} returns the randomly-chosen heap, \cc{min_affinity}
tries to acquire the lock of that heap to run its min process.  If it
fails to lock (because another core just locked it), \cc{min_affinity}
selects another random heap and starts over again (and may select
\cc{cp}). If \cc{select_affinity} acquires the heap's lock
successfully, it returns that heap's min process for the core to run.

The number of retries is likely to be small, because the number of
heaps is $c \times d$. So, \cc{min_affinity} is likely to quickly find a heap
that hasn't been locked by another core, even if all other cores have
locked some heap.

Core-affinity selection favors cache performance of high-weight
processes over low-weight processes. The highest-weight group with a
number of processes in the order of number of cores will often see its
processes run on cores they ran previously on, because high-weight
processes will have a low virtual time and be at the front of some
heap.  On the other hand, lower-weight processes are unlikely to see
much core affinity.  This trade-off reflects that higher-weight
processes are more important than lower-weight processes.  Note,
however, that core-affinity selection preserves global weights and
lower-weight processes will get their CPU share.
