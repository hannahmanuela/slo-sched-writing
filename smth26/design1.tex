\section{Design of \sys}
\label{sec:global}

\sys is a new design for scheduling processes that enforces group
weights globally across cores.  It assigns each runnable process a
global virtual time based on its group weight. \sys maintains a heap
of these processes based on their virtual runtimes, with the process
with lowest virtual time at the front of the heap
(\autoref{sec:vt}). To implement this heap in a scalable manner, \sys
uses an {\it relaxed} global heap and may select a process that has a
virtual time higher than the global minimum virtual time
(\autoref{sec:relaxed}).  Finally, if several processes of the same
group are runnable, \sys selects the one that it has run recently to
provide core affinity so that the selected process runs with a warm
L1/L2 caches.

\subsection{Global virtual time}
\label{sec:vt}

\begin{figure}
\input{code/group}
\caption{The state for each group}
\label{fig:group}
\end{figure}

\autoref{fig:group} shows the state that \sys maintains for each
cgroup: \cc{weight}, the groups weight, \cc{vruntime}, which is the
next available virtual runtime for a process in the cgroup to run, and
\cc{min_vt_deq}, which is the virtual runtime when a group goes to
  ``sleep'' (i.e., when all group's processes are sleeping).

\begin{figure}
\input{code/enq}
\caption{Enqueue a process}
\label{fig:enq}
\end{figure}

When a process of in a group becomes runnable, \sys assigns it a
virtual time and inserts it in the global heap, as show in
\autoref{fig:enq}. Consider a \cc{tick_length} of 1000us and two
groups, $g_1$ with weight 10 and $g_2$ with weight 20. When a process
of $g_1$ becomes runnable it will get assigned the current group's
virtual time and increases it by 100 ($= 1000/10$).  If concurrently,
on another core, another process of $g_1$ becomes runnable it will be
assigned a virtual time that is 100 larger than the first process, and
$g_1$'s virtual time will be incremented by 100, ensuring the
processes are correctly spaced in virtual time by 100.

When a process of $g_2$ becomes runnable, it will increases its
group's virtual time by 50 ($= 1000/20$), and the processes of $g_2$
will be spaced in virtual time by 50.  If we look over a window of a
1000 in virtual time, $g_1$ will run 10 times and $g_2$ will run 20
times, reflecting their relative weights of 10 and 20.

\begin{figure}
\input{code/adjust}
\caption{Adjust virtual runtime when a process yield before a full clock tick.}
\label{fig:adjust}
\end{figure}

If a process yields its core before \cc{tick_length}, \sys adjusts the
group's virtual time to reflect that the process didn't use the full
tick, as shown in \autoref{fig:adjust}.  The adjustment moves move the
group's virtual time backwards so that the next process of the group
inherits the time that the first process didn't use.

\begin{figure}
\input{code/lag}
\caption{Set group's virtual time when it becomes runnable.}
\label{fig:lag}
\end{figure}

When the last running process of a group goes to sleep, \sys remembers
the minimum virtual time of the heap in the process group's
\cc{min_vt_deq}. It does so to set the group's \cc{vruntime} when
the group becomes runnable again, as shown in \autoref{fig:lag}.  When
the group becomes runnable, the global virtual runtime may have moved
ahead since the group went to sleep. To account for this, \sys sets
the group's virtual time to the current minimum global virtual time,
adjusted for how much the group was ahead or behind the global minimum
when the group dequeued.

The way \sys computes virtual time is similar to Linux's fair-share
scheduler with the main difference being that \sys must compute the
virtual time globally, and, handle multiple cores concurrently
assigning virtual times to processes of the same group.  In Linux
virtual time is per run queue (and not global), and updated only by
one core. As a result, Linux, for example, doesn't need to update the
group's virtual on enqueue, doesn't need to adjust it if a process
runs shorter than its full tick, and doesn't need to set a group's
virtual time when it wakes up.

\subsection{\mheap: an relaxed global heap}
\label{sec:relaxed}

\sys's heap of runnable processes is often written by many cores
concurrently: cores remove the process with the minimal virtual time
from the heap and insert runnable processes in the heap based on their
virtual times.  To avoid operations contending on the global heap, \sys
uses a multi-heap design with a relaxed global minimum, which is based
on relaxed concurrent priority queues~\cite{multiqueue:2015}.

In concurrent priority queues, a core inserts an element in a random
queue out of $n \times d$ queues, where $n$ is the number of cores and
$d$ is a small constant (typically 2).  To find the minimum element to delete,
concurrent priority queues chooses two random queues and removes the
smallest one. The intuition behind this idea is from randomized load
balancing: choosing two randomly-chosen machines and add a ball to the
least-loaded one results in a maximum load very close to the average
load with high
probability~\cite{mitzenmacher:power,berenbrink:balanced}.

Concurrent priority queues are attractive for implementing \sys
because they allow for concurrent write operations.  Each queue has
its own lock and cores retry choosing a random queue if the chosen
queue is locked by another core.  Williams et al. show---in theory and
in practice---that concurrent priority queues can achieve throughout
that scales with increasing number of cores and that the rank error (the
distance of the deleted element to the best element) is in expectation
$O(c \log c)$ with high probability~\cite{williams:multiqueue}.

\begin{figure}
\input{code/schedule}
\caption{The schedule function}
\label{fig:schedule}
\end{figure}





