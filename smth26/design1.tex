\section{Design of \sys}
\label{sec:global}

\sys is a new design for scheduling processes that enforces group
weights across cores, implementing the specification of \cgroups{}
weights as stated in its documentation.  \sys assigns each runnable
process a global virtual time based on its group weight. \sys
maintains a heap of the runnable processes based on their virtual
runtimes, with the process with lowest virtual time at the front of
the heap (\autoref{sec:vt}).

To implement this heap in a scalable manner, \sys uses a {\it relaxed}
heap based on multiqueues~\cite{multiqueue:2015}. Multiqueues uses
many queues to implement a global priority queue. It finds the minimum
element by sampling two queues and choosing the smallest
element. Multiqueues is relaxed in the sense that the chosen element
might be larger than the true global minimum, but the random sampling
ensures that the chosen element is close to the global minimum (with
high probability).  \sys uses multiqueues to maintain the heap of
processes sorted by virtual time. Furthermore, \sys leverages the
existences of multiple queues to allow a core to select a process that
it has run recently so that the process runs with a warm L1/L2 cache
(\autoref{sec:relaxed}).

\subsection{Global virtual time}
\label{sec:vt}

\begin{figure}
\input{code/group}
\caption{The state for each group.}
\label{fig:group}
\end{figure}

\autoref{fig:group} shows the state that \sys maintains for each
cgroup: \cc{weight}, the groups weight, \cc{vruntime}, which is the
next available virtual runtime for a process in the cgroup to run, and
\cc{min_vt_deq}, which is the virtual runtime when a group goes to
  ``sleep'' (i.e., when all group's processes are sleeping).

\begin{figure}
\input{code/enq}
\caption{Calculate the virtual for a process and insert it in heap.}
\label{fig:enq}
\end{figure}

When a process of in a group becomes runnable, \sys assigns it a
virtual time and inserts it in the global heap, as show in
\autoref{fig:enq}. Consider a \cc{TICK_LENGTH} of 1000us and two
groups, $g_1$ with weight 10 and $g_2$ with weight 20. When a process
of $g_1$ becomes runnable it will get assigned the current group's
virtual time and increases it by 100 ($= 1000/10$).  If concurrently,
on another core, another process of $g_1$ becomes runnable it will be
assigned a virtual time that is 100 larger than the first process, and
$g_1$'s virtual time will be incremented by 100, ensuring the
processes are correctly spaced in virtual time by 100.

When a process of $g_2$ becomes runnable, it will increases its
group's virtual time by 50 ($= 1000/20$), and the processes of $g_2$
will be spaced in virtual time by 50.  If we look over a window of a
1000 in virtual time, $g_1$ will run 10 times and $g_2$ will run 20
times, reflecting their relative weights of 10 and 20.

\begin{figure}
\input{code/adjust}
\caption{Adjust virtual runtime when a process yield before a full clock tick.}
\label{fig:adjust}
\end{figure}

If a process yields its core before \cc{TICK_LENGTH}, \sys adjusts the
group's virtual time to reflect that the process didn't use the full
tick, as shown in \autoref{fig:adjust}.  The adjustment moves the
group's virtual time backwards so that the next process of the group
inherits the time that the first process didn't use.

\begin{figure}
\input{code/lag}
\caption{Set group's virtual time when it becomes runnable.}
\label{fig:lag}
\end{figure}

When the last running process of a group goes to sleep, \sys remembers
the minimum virtual time of the heap in the process group's
\cc{min_vt_deq}. It does so to set the group's \cc{vruntime} when the
group becomes runnable again, as shown in \autoref{fig:lag}.  When the
group becomes runnable, the virtual runtime may have moved ahead since
the group went to sleep. To account for this, \sys sets the group's
virtual time to the heap's current minimum virtual time, adjusted for
how much the group was ahead or behind the heap's minimum when the
group dequeued.

The way \sys computes virtual time is similar to Linux's fair-share
scheduler with the main difference being that \sys must compute the
virtual time across cores, and, handle multiple cores concurrently
assigning virtual times to processes of the same group.  In Linux
virtual time is per run-queue (and not across cores), and updated only
by one core. As a result, Linux, for example, doesn't need to update
the group's virtual on enqueue, doesn't need to adjust it if a process
runs shorter than its full tick, and doesn't need to set a group's
virtual time when it wakes up.

Because \sys maintains one heap across cores, however, it avoids the
scheduling anomalies from \autoref{s:problem}. If a high-weight
process becomes runnable, \sys will assign it a low virtual time and
it will be at the front of the heap.  When a core is interrupted or at
the end of its clock tick, and looking for the next process to run,
the core will find the high-weight process in the heap.

\subsection{\sys's relaxed heap}
\label{sec:relaxed}

\sys's heap of runnable processes is written by many cores
concurrently: cores remove the process with the minimum virtual time
from the heap to run it and insert runnable processes in the heap
based on their virtual times.  A simple correct plan would to protect
the heap with a lock, but the lock would serialize all heap operations
and would quickly become contended.  To avoid serializing all heap
operations and avoid lock contention, \sys uses relaxed
multiqueues~\cite{multiqueue:2015, williams:arxiv,williams:multiqueue}.

\paragraph{Multiqueus.} With multiqueues, a core inserts an element in
a random queue out of $c \times d$ queues, where $c$ is the number of
cores and $d$ is a small constant (typically 2).  To find the minimum
element to remove, multiqueues chooses two random queues and removes
the smallest one. The intuition behind this idea is from randomized
load balancing: choosing two randomly-chosen machines and add a ball
to the least-loaded one results in a maximum load close to the average
load with high
probability~\cite{mitzenmacher:power,berenbrink:balanced}.

\sys uses a multiqueue to store all runnable processes sorted by
virtual time.  Multiqueues are attractive for implementing \sys
because multiqueues allow for concurrent heap operations.  Each queue
has its own lock and operations on different queues can execute
concurrently. Furthermore, cores retry choosing a random queue if the
chosen queue is locked by another core. Since there are $c \times d$
queues, even if all other cores have a queue locked, the one remaining
core is likely to find an unlocked queue quickly.  Williams et
al. show---in theory and in practice---that multiqueues can achieve
throughout that scales with increasing number of cores and that the
rank error (the distance of the deleted element to the best element)
is in expectation $O(c \log c)$ with high
probability~\cite{williams:multiqueue}.

\paragraph{Core affinity.} Multiqueues don't provide core-affinity for processes.
Because all processes are totally ordered by virtual time and cores
randomly sample queues, it is likely that if one core has run a
process, that process will run next on a different core. This results
in poor L1/L2 cache performance for all processes.  \sys modifies the
selection procedure to realize some core affinity while preserving
global weights.

Specifically, \sys modifies the remove operation of multiqueues to
allow cores to choose a process that it has run recently so that the
process can benefit from a warm L1/L2 cache.  A core remembers the
last process it ran, which, if it is runnable again, is in some random
heap.  If the process is at the front of that heap, \sys samples one
random heap.  If the process in the sampled heap belongs to the same
cgroup (i.e., has the same weight) as the core's recently-run process
(or the process has a higher virtual time),
\sys removes the recently-run process and runs it.  This process may
have a higher virtual time than the not-selected process, but since
the not-selected process is at the front of a heap, another core is
likely to select it soon.

\begin{figure}
\input{code/schedule}
\caption{Selecting with affinity}
\label{fig:schedule}
\end{figure}

\autoref{fig:schedule} shows core-affinity selection in more
detail. If \cc{cp}, the process that the core has run recently, has
already been selected by another core, \cc{min_affinity} returns
immediately and falls back to sampling two heaps without consideration
of affinity, as in multiqueus.

Next, \cc{min_affinity} checks if \cc{cp} is at the front of the
random heap that it was inserted in. If so, it acquires the lock of
that heap and re-checks if \cc{cp} is still at the front, in case
another core slipped in between the first check and the lock acquisition.
Next, \cc{min_affinity} chooses a random heap and invokes
\cc{select_affinity}, which checks if the min process of the
randomly-chosen heap belongs to the same cgroup or has a higher
virtual time. If so, the \cc{min_affinity} ignores the randomly-chosen
heap and returns \cc{cp}. The process \cc{cp} will now run on the same
core as it ran recently on and with a warm L1/L2 cache.

If \cc{select_affinity} returns the randomly-chosen heap, \cc{min_affinity}
tries to acquire the lock of that heap to run its min process.  If it
fails to lock (because another core just locked it), \cc{min_affinity}
selects another random heap and starts over again (and may select
\cc{cp}). If \cc{min_affinity} acquires the heap's lock
successfully, it returns that heap's min process for the core to run.

The number of retries is likely to be small, because the number of
heaps is $c \times d$. So, \cc{min_affinity} is likely to quickly find a heap
that hasn't been locked by another core, even if all other cores have
locked some heap.

Core-affinity selection favors cache performance of high-weight
processes over low-weight processes. The highest-weight group with a
number of processes in the order of number of cores will often see its
processes run on cores they ran previously on, because high-weight
processes will have a low virtual time and be at the front of some
heap.  On the other hand, lower-weight processes are unlikely to see
much core affinity if many high-weight processes are runnable.  (Of
course, if the high-weight application doesn't receive much load,
lower-weight processes will benefit from core affinity.)  This
trade-off reflects that higher-weight processes are more important
than lower-weight processes. Note, however, that core-affinity
selection preserves global weights and lower-weight processes will get
their CPU share.
