%-------------------------------------------------------------------------------
\section{Problem}\label{s:problem}
%-------------------------------------------------------------------------------

The problem is that Kubernetes does not enforce the described interface. We find
that reservations are not enforced: services with reservations can see
performance impacts from other services, both from those that are only supposed
to be running opportunistically, \ie{} bursting Burstable services and
BestEffort workloads, but also from those that are not, \ie{} other Guaranteed
ones. We track the issue down to Linux' \cgroups{}, and show that its interface
is not conducive to supporting the QOS guarantees outlined.


\subsection{Burstable pods affect Guaranteed pods}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-lc-burst.png}
    \caption{In Kubernetes, running a Burstable server2 alongside the Guaranteed
    server1 affects server1's performance}\label{fig:kubernetes-lc-burst}
\end{figure}

We use as a case study a small but realistic social network web application,
which we run using Kubernetes. We run alongside it a CPU-bound server, service2,
as a Burstable pod. They each request two CPUs, and are pinned to the same four
cores. \autoref{fig:kubernetes-lc-burst} shows the impact on the web server's
latency of starting load on the other Burstable service: average latency jumps
up by $\sim$2ms, and 99th pctile by > 6ms.

Understanding why this is happening requires understanding how Kubernetes
enforces the CPU isolation between pods. Kubernetes uses the \cgroups{} weight
interface to enforce pods' CPU requests: It makes sure that machines are never
oversubscribed on requested CPU, and then uses weights to split up the cores on
the machine. Burstable and Guaranteed pods are given a weight that is calculated
based on the number of cores the machine has and how many CPUs the pod
requested. For instance, on a 56 core machine we found that a pod that requested
4 cores was given a weight of 157, whereas one that request 2 CPUs was given a
weight of XX.

We find that the weight interface fails to enforce these pods' requests because
of one key observation: \textit{weights are a local property}. Linux maintains a
separate runqueue on each core, in order to avoid the overheads of accessing
global state for every scheduling decision. Within each runqueue, Linux's
scheduler works to maintain the correct ratio of received CPU time at each
scheduling; but Linux does not enforce the weight ratios across cores.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-lc-burst-problem.png}
    \caption{When server1 is running alone, it has all the cores to itself, but
    after server2 starts it gets only half of whatever core it runs on, even if
    that is a fraction of the cores available and server2 is running uncontended
    on the other cores}\label{fig:schedviz-lc-burst-problem}
\end{figure}


This is a problem because Burstable and Guaranteed pods share cores where they
both have threads based on weight, irrespective of who is running how much on
other cores. In particular, Server1 has less load and thus less runnable
threads. However, on the few cores where its threads are runnable, it will have
to share that core equally; even while the server with higher load runs
uncontended on other cores. \autoref{fig:schedviz-lc-burst-problem} shows this
occuring in a trace from the experiment, which we visualize using
schedviz~\cite{schedviz-tool}. The process running on each core is shown as an
oval, and queued processes are shown as rectangles below; the x-axis is time and
the y-axis shows the 4 cores the experiment is running on. On the left hand side
of the trace, server1 can run uncontended on any core when requests come in.
Then server2 starts, and has a high load and thus immediately starts running on
all four cores, because it is Burstable. When a request from server1 comes in,
the thread processing that request has to share the core equally with threads
from server2, even while server2 gets other cores all to itself.




\subsection{BestEffort pods run more than they should}

Kubernetes uses the smallest weight possible, 1, to run BestEffort pods. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-unedited.png}
    \caption{Running a BestEffort affects the server's
    performance}\label{fig:kubernetes-unedited}
\end{figure}

\autoref{fig:kubernetes-unedited} shows the result of running a best effort
workload, using the BestEffort class, alongside the web server. The top graph of
plots the end-to-end latency of an endpoint that gets a users feed and applies a
moderation; the bottom graph shows the throughput of a best effort image resize
job. After the image resize job starts running, the mean response latency of the
web application jumps from $\sim$7ms to $\sim$15ms, and the 99th percentile
latency from $\sim$10ms to $\sim$25ms.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-be-problem.png}
    \caption{Core 6 runs an image resize process, unaware that cores 2, 4, and 8
    all have runnable and queued server threads}\label{fig:schedviz-be-problem}
\end{figure}

The reason this happens is because \textit{one core is running a best effort
process, while threads of processes with resource reservations are queued on
another}. \autoref{fig:schedviz-be-problem} shows this happening: We see that on
core 6, the red process that is running for the whole 10ms is a thread of the
image resize job, while server threads, shown in varying shades of blue, are
queued on the other cores. 

The core of this issue goes back to the local nature of \cgroups{} weight
enforcement. The observed inversion happens because the threading model of the
server interacts with the per-core runqueues. The server uses a pool of worker
threads (one per client). The number of threads that the server has is larger
than the number of cores, which means that each core has multiple server
threads. It occasionally occurs that all the current requests are on server
threads that happen to be on only three of the four available cores, which
leaves one runqeueue with only idle server worker threads. This leads to an
inversion, where the core that has no runnable high weight server threads and
thus runs a low weight image resize thread, even while other cores have queued
high weight processes.

Load balancing eventually remedies the inversion by migrating runnable
high-weight threads to the under-loaded core. However, load balancing runs
significiantly less frequently than scheduling does, at least during high load.
How often load balancing runs in general is a complicated number that is
dependent on how close the two cores are in the CPU architecture hierarchy, as
well as how loaded the machine is.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/srv-bg-weight-cmp-low.png}
    \caption{Changing the weight of the server beyond 100 has little impact on
    how much the weight 1 best effort task interferes with
    it}\label{fig:srv-bg-weight-cmp}
\end{figure}

\hmng{Not sure if this is still the right place to do this then?}This inversion
has a large effect, to the point of making weight ratios above a certain range
ineffective across cores. We demonstrate this with a microbenchmark, which runs
a simple CPU-bound server with a pool of worker threads. We run an open-loop
client on a remote machine, and then start two best effort workloads doing image
resizing. We put the server and the image resize job each in their own
\cgroups{} group, and pin them to the same set of four cores. The image resize
job always has weight 1, and we vary the weight of the server.
\autoref{fig:srv-bg-weight-cmp} shows that using a bigger weight split has no
impact on the latency impact of the best effort task. For all the weights, at a
$\sim$85\% baseline utilization of the server the server's mean latencies spike
up from steady at around 6ms to $\sim$15ms, and much higher for 99th percentile
latencies. At a baseline utilization of 95\%, those numbers increase to up to
40ms for the the mean latency, and 80ms for the tail.



\subsection{Guaranteed pods affect other Guaranteed pods}

We find that even two Guaranteed pods affect each others performance. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-lc-guar.png}
    \caption{In Kubernetes, running a Guaranteed server2 alongside the
    Guaranteed server1 affects server1's
    performance}\label{fig:kubernetes-lc-guar}
\end{figure}

We run the same two servers: one web server and one CPU-bound server, both now
in the Guaranteed class. \autoref{fig:kubernetes-lc-guar} shows the result:
average latency jumps up by $\sim$3ms, and 99th pctile by > 10ms.


The reason this happens is also related to how Kubernetes uses \cgroups{} to
enforce limits on pods. Kubernetes uses cpu.max to do so, but the interface to
\cgroups{} cpu.max is already different from that of Kuberentes' milliCPUs:
\cgroups{} asks instead for a runtime $r$ and a period $p$, and ensures that the
group never gets more than $r$ CPU time per $p$ time period. Because
multi-threading is a thing, it is possible that $r>p$. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-lc-guar-problem.png}
    \caption{When server1 is running alone, it has all the cores to itself, but
    after server2 starts it gets only half of whatever core it runs on, even if
    that is a fraction of the cores available and server2 is running uncontended
    on the other cores}\label{fig:schedviz-lc-guar-problem}
\end{figure}

Enforcing a limit of 2 vCPUs is different than enforcing that a group only get
2ms of CPU time every 1ms. If a pod is restricted to 2 vCPUs, then it can run up
to two things at the same time, and thus use only up to two cores. However, if a
group is limited to 2ms every 1ms, it could also fill its quota by running four
things on four cores for half a ms. In the time when one group is running on all
four cores, Kubernetes relies on weights to separate the two, which can lead to
significant delays for other services, as we saw.
\autoref{fig:schedviz-lc-guar-problem} shows this happening in a trace from the
experiment from \autoref{fig:kubernetes-lc-guar}. We can see the phases of
service2 threads in grey, which run for a while on all four cores, disrupting
the blue server1 threads, before being throttled, during which time server1
threads can run uninterrupted and otherwise the cores lie idle.



% \subsection{Enforcing weights across cores is
% hard}\label{ss:problem:cross-core-hard}

% A weight-based interface is at odds with machine-wide policy enforcement. In
% order to globally enforce a processes weight, the scheduler would need to
% look at every cores' runqueues at every scheduling decision. 

% This check needs to happens at every tick because, in a weight-based system, the
% definition of what process should be running changes over time. If a core has
% one high-weight and one low-weight group, before running the low-weight group it
% needs to know if any other cores have recently run a different process within
% that group, because that would affect how much time it is owed on this core.

% Enforcing weights across cores is expensive: looking at remote queues at each
% scheduling decision removes the benefit of having local runqueues. Inter-core
% communication and having every core look at every other cores' runqueues can
% lead to contention, which has been demonstrated to be a bottleneck for
% performance at scale~\cite{afaas}. This means that the overheads of maintaining
% a weight split as a global invariant can quickly become prohibitive.

% On top of the communication cost, managing and calculating global shares is
% complex: calculating whether a given process is owed time globally requires
% knowing the total weight across all cores as well as the sum of time that all
% the processes in the group have gotten. Such a calculation would include
% complicated accounting that takes into account, amount other things, different
% cores' virtual times, processes' weights, groups, and limits. Just within
% runqueues the accounting is already so complicated that comments in the Linux
% source include function derivations and equation rewriting.


% \subsection{Weights interact poorly with tick-based
% scheduling}\label{ss:problem:quantum}

% Even on a single runqueue, in a weight based scheme best effort processes need
% to get a fair share of the CPU. The problem is that, when this happens, the BE
% process will interrupt any running LC process for a whole tick. In Linux,
% hardware ticks are 4ms long. This means that a thread that has a CPU reservation
% and is processing a request may be interrupted for up to 4ms by a best effort
% process, provided the latter runs that long before blocking. 4ms can be a large
% amount of time for microservice workloads, whose SLOs are often in the low
% double digit or even single digit ms realm.~\cite{in-the-plex, sigmaos}



