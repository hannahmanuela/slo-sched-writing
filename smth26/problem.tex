%-------------------------------------------------------------------------------
\section{Problem}\label{s:problem}
%-------------------------------------------------------------------------------

The observed failure to isolate between the LC web application and the BE image
resize happens because BEs sometimes run uncontended on one core while another
has queued and waiting LC threads (\autoref{ss:problem:bes-run}). This is
because weights are only strictly enforced within runqueues, which are per-cpu
(\autoref{ss:problem:weights-local}).


\subsection{Weights don't isolate}\label{ss:problem:bes-run}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-unedited-low.png}
        \caption{Low load stetting, utilization before starting the BE tasks is
        around 85\%}\label{fig:srv-bg-unedited-low}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[b]{0.49\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-unedited-high.png}
        \caption{High load setting, utilization before starting the BE tasks is
        around 95\%}\label{fig:srv-bg-unedited-high}
    \end{subfigure}
    \vspace{4pt}
    \caption{Latencies of the server and iteration counts of the background
    tasks in different load scenarios. Note the different y axis limits. The
    upper graphs show end-to-end request latencies, and the bottom graph is a
    histogram of completed iterations of the BE tasks}\label{fig:srv-bg-unedited}
\end{figure}

\autoref{fig:kubernetes-unedited} includes a complicated software stack of
Kubernetes, Python, and the libraries used to expose the service's API and
manage worker threads. All of this has performance implications, and we can see
that even before the workers start there are large latency spikes of up to 30ms.
In order to isolate why the jump in latency is happening when the BEs run from
the many incidentals of this experiment, we reproduce the effects in a simpler
benchmark. We run a simple cpu-bound spin server with a pool of worker threads
that we hit with an open-loop remote client, and then start two BE workloads
doing image resizing. We put the LC server and the BE resize job each in their
own \cgroups{} group with weights 10000 and 1 respectively, and pin them to the
same set of cores. \autoref{fig:srv-bg-unedited} shows the increase in latencies
of the LC server at two different baseline utilization levels. We see that even
in the lower baseline utilization case, where the server alone uses 85\%, mean
latencies spike up from steady at around 6ms to as high as 20ms, and much higher
for 99th percentile latencies.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-problem.png}
    \caption{Each thread is a different color. Circles represent which
    thread is running on that core, while rectangles underneath show waiting but
    runnable threads
    }\label{fig:schedviz-problem}
\end{figure}

Using this simplified experiment, we analyze perf traces of the scheduling
decisions, and visualize the trace using schedviz~\cite{schedviz-tool}, in order
to understand where the latency impact comes from.
\autoref{fig:schedviz-problem} shows a 10ms outtake of the resulting image. The
process running on each core is shown as a an oval, and queued processes are
shown as rectangles below; the x `axis' is time and the y `axis' shows the 4
cores the experiment is running on. The root of the undesirable behavior is: on
core 6, the red process that is running the whole time is a BE process, whereas
LC threads, shown in varying shades of blue, are queued on the other cores. The
takeaway here is that the failure mode we observe is that \textit{one core is
running a BE process, while an LC process is waiting on another}.

\subsection{Weights are enforced only locally}\label{ss:problem:weights-local}

The reason this undesirable behavior happens is that Linux maintains a separate
runqueue on each core, in order to avoid the synchronization overheads of
accessing global state for every scheduling decision. Within each runqueue,
Linux's scheduler works to maintain the correct ratio of received CPU time at
each scheduling; but Linux does not enforce the weight ratios across cores. This
leads to the above failure mode, where one core has no runnable high weight
processes and thus runs a low weight one, whereas another core has queued high
weight processes.

\subsection{Enforcing weights across cores is
expensive}\label{ss:problem:cross-core-hard}

The root of the problem goes deeper than just a poor implementation: a
weight-based interface is at odds with machine-wide policy enforcement. In order
to strictly and globally enforce a processes weight, the scheduler would need to
synchronize at every scheduling decision: calculating whether a given process is
owed time globally requires knowing the total weight across all cores as well as
the sum of time that all the processes in the group have gotten. This
effectively removes the benefit of local runqueues, since the scheduler needs to
synchronize at every decision it makes. In increasingly multi-core and
multi-NUMA machines, synchronization is expensive; other work has found that
kernel lock contention is a bottleneck to performance at scale~\cite{afaas}.
This means that the overheads of maintaining a weight split as a global
invariant can quickly become prohibitive.


\subsection{Weights interact poorly with tick-based
scheduling}\label{ss:problem:quantum}

Even on a single runqueue, using very large weight differentials as an interface
to isolate LC and BE doesn't work well because preemption is tick-based. In a
weight based scheme, BE processes also get a fair share of the CPU. The problem
is that, when this happens, the BE process will interrupt any running LC process
for a whole tick. In Linux, hardware ticks are 4ms long. This means that an LC
thread processing a request may be interrupted for up to 4ms, provided the BE
process runs for that long before blocking. 4ms can be a large amount of time
for microservice workloads, whose SLOs are often in the low double digit or even
single digit ms realm.~\cite{in-the-plex, sigmaos}



