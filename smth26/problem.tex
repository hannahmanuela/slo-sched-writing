%-------------------------------------------------------------------------------
\section{Problem}\label{s:problem}
%-------------------------------------------------------------------------------

This section shows that the observed behavior when running
applications on Kubernetes violates the expectation of stable
performance for high QOS class pods: Guaranteed as well as Burstable
pods see latency increases based on load on lower classes. We
track the issue down to Linux' \cgroups{}, and show that its
implementation is not conducive to supporting the classes, and
violates the behavior that the \cgroups{} documentation specifies.

We use as a case study a small but realistic social network web application,
which we run using Kubernetes. We use a CPU-bound server for Guaranteed or
Burstable, and an image resize job for the BestEffort QOS class. In all cases,
we run two pods, each requesting 2 cores, on the same four cores.

\subsection{BestEffort pods impact Burstable and Guaranteed performance}

A goal of the QOS classes is that the lowest class, BestEffort, be
invisible to higher classes. BestEffort tasks need to be preempted as soon as a
higher-class pod has anything to run. If this is correctly enforced, Guaranteed
and Burstable pods should be able to maintain steady average and tail latency
in the face of changing load on a BestEffort job.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-unedited.png}
    \caption{Running a BestEffort affects the server's
    performance}\label{fig:kubernetes-unedited}
\end{figure}

However, we find that running a BestEffort job does in fact impact the
performance of higher class pods. \autoref{fig:kubernetes-unedited} shows the
result of running a best effort workload, using the BestEffort class, alongside
the web server, running as a Burstable pod. The top graph plots the end-to-end
latency of an endpoint that gets a users feed and applies a moderation; the
bottom graph shows the throughput of a best effort image resize job. After the
image resize job starts running, the mean response latency of the web
application jumps from $\sim$7ms to $\sim$15ms, and the 99th percentile latency
from $\sim$10ms to $\sim$25ms. The graph looks the same if the web server is
running as a Guaranteed pod. This clearly violates the goal that Guaranteed and
Burstable pods get access to reserved CPUs when they could use them.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-be-problem.png}
    \caption{Core 6 runs an image resize process, unaware that cores 2, 4, and 8
    all have runnable and queued server threads}\label{fig:schedviz-be-problem}
\end{figure}

To understand the cause of the above violation,
we visualize the
trace of the experiment using schedviz~\cite{schedviz-tool}. What we find is
that frequently \textit{one core is running a best effort process, while threads
of processes with resource reservations are queued on another}.
\autoref{fig:schedviz-be-problem} shows an outtake from the trace. The process
running on each core is shown as an oval, and queued processes are shown as
rectangles below; the x-axis is time and the y-axis shows the 4 cores the
experiment is running on. We see that on one of the cores, the red process that
is running for the whole 10ms is a thread of the image resize job, while server
threads, shown in varying shades of blue, are queued on the other cores.

Understanding why this is happening requires understanding how Kubernetes
enforces the CPU isolation between pods. Kubernetes uses the \cgroups{} weight
interface to enforce pods' CPU requests: It makes sure that machines are never
oversubscribed on requested CPU, and then uses weights to split up the cores on
the machine.

Burstable and Guaranteed pods are given a weight that is calculated based on the
number of cores the machine has and how many CPUs the pod requested. For
instance, on a 56 core machine we found that a pod that requested 4 cores was
given a weight of 157, whereas one that request 2 CPUs was given a weight of 79.
Kubernetes uses the smallest weight possible, 1, to run BestEffort pods.

The key fact relevant is that Linux implements \cgroups{} weight as a
{\it local} property: Linux maintains a separate run-queue on each core, and within
each run-queue, Linux's scheduler works to maintain the correct ratio of received
CPU time at each scheduling. Linux, however, does not enforce the weight ratios across
cores.

The observed inversion happens then because the threading model of the server
interacts with the per-core run-queues. The server uses a pool of worker threads
(one per client). The number of threads that the server has is larger than the
number of cores, which means that each core has multiple server threads. It
occasionally occurs that all the current requests are on server threads that
happen to be on only three of the four available cores, which leaves one
runqeueue with only idle server worker threads. This leads to an inversion,
where the core that has no runnable high weight server threads and thus runs a
low weight image resize thread, even while other cores have queued high weight
processes.


\subsection{Burstable pods affect Guaranteed pods}

Another relevant promise of the QOS-class-based interface is that two pods with
reservations not affect each other. Because Kubernetes never oversubscribes on
requests, that means that every pod with a reservation should be able to get
access to the CPUs it requested, and use them. We would then expect that a
Guaranteed pod is able to maintain steady average and tail latency under steady
load, irrespective of the load on a Burstable pod running alongside it.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/kubernetes-lc-burst.png}
    \caption{In Kubernetes, running a Burstable service2 alongside the Guaranteed
    service1 affects service1's performance}\label{fig:kubernetes-lc-burst}
\end{figure}

To test this expectation, we run experiments using two servers: service1 runs
the same web server, and service2 is a simple CPU-bound server. 
We run the web server in the Guaranteed class, and service2 in the Burstable
class. Surprisingly, we see a significant latency impact under high load.
\autoref{fig:kubernetes-lc-burst} shows the web server's latency and the overall
utilization, before and after starting load on service2. We see that average
latency jumps from $\sim$6ms to $\sim$9.5, and the 99th pctile from 7ms to 22.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/schedviz-lc-burst-problem.png}
    \caption{When service1 is running alone, it has all the cores to itself, but
    after service2 starts it gets only half of whatever core it runs on, even if
    it isn't using all the cores available and service2 is running uncontended
    on the other cores}\label{fig:schedviz-lc-burst-problem}
\end{figure}

\autoref{fig:schedviz-lc-burst-problem} shows an outtake from the trace of the
experiment. Threads of service1 are colored in, threads are serer2 are shown in
shades of grey. On the very left of the trace, service1 can run uncontended on
any core when requests come in. Then service2 starts, and has a high load and
thus immediately starts running on all four cores, because it is Burstable. When
a request from service1 comes in, the thread processing that request has to
share the core equally with threads from service2, or other service1 threads,
while while service2 gets other cores all to itself.

This means that Burstable pods have a performance impact on the average and tail
latency of Guaranteed ones because both share cores where they both have threads
as equals, irrespective of who is running how much on other cores. In
particular, service1 has less load and thus less runnable threads. However, on
the few cores where its threads are runnable, it will have to share that core
equally; even while the server with higher load runs uncontended on other cores.
We conclude that the issue boils down to the locality of weights again: if the
core with both servers' threads knew service2 had other cores all to itself it
could compensate, but it doesn't know, so it treats both equally.

\subsection{\cgroups{} in isolation }

\fk{results from microbenchmark with 5 processes and 2 cores}

\subsection{Existing Linux mechanisms that aren't fixes}

\subsubsection{Load balancing}

Linux performs periodic load balancing, where it works to equalize the weight
across different cores. However, load balancing runs significiantly less
frequently than scheduling does, especially during high load. How often load
balancing runs in general is a complicated number that is dependent on how close
the two cores are in the CPU architecture hierarchy, as well as how loaded the
machine is.  \fk{load balancer balances load, not weights}
This means that in the context of a stable set of long-running
processes, load balancing can ensure that no two cores have wildly different
total weights and thus that the time each thread gets on each core reflects the
time it should get overall. However, in the context of short-lived request
processing with a constantly changing set of runnable worker threads, load
balancing is not enough.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/srv-bg-weight-cmp-low.png}
    \caption{Changing the weight of the server beyond 100 has little impact on
    how much the weight 1 best effort task interferes with
    it}\label{fig:srv-bg-weight-cmp}
\end{figure}

\subsubsection{Bigger weight splits}

As we saw in our experiment, Kubernetes uses a weight split of around 79:1,
whereas Linux supports weights in the range of [10000,1]. We show that a larger
weight split does not improve the inrease in average or tail latency. We
demonstrate this with a microbenchmark, which runs a simple CPU-bound server
with a pool of worker threads. We run an open-loop client on a remote machine,
and then start two best effort workloads doing image resizing. We put the server
and the image resize job each in their own \cgroups{} group, and pin them to the
same set of four cores. The image resize job always has weight 1, and we vary
the weight of the server. \autoref{fig:srv-bg-weight-cmp} shows that using a
bigger weight split has no impact on the latency impact of the best effort task.
For all the weights, at a $\sim$85\% baseline utilization of the server the
server's mean latencies spike up from steady at around 6ms to $\sim$15ms, and
much higher for 99th percentile latencies. At a baseline utilization of 95\%,
those numbers increase to up to 40ms for the the mean latency, and 80ms for the
tail.

\subsubsection{Alternatives to \cgroups{}}

Weights remains the \cgroups{} interface of choice for Kubernetes as
well as others like Firecracker and libvirt, but Linux also supports
other mechanisms for scheduling.  The Linux scheduler is hierarchical:
it supports different \textit{scheduling classes}, within which there
might be different \textit{policies}. The \normalclass{} scheduling
class is the one most people know---it used to run a Completely Fair
Scheduler (CFS) and now runs a version of Earliest Eligible Virtual
Deadline First (EEVDF). It is the default scheduler, and \cgroups{}
weight is only effective within the \normalclass{} class.

\schedidle{} is Linux's one answer to doing a better job of separating best
effort tasks from those with reservations.
\schedidle{} is a scheduling policy that lives within the \normalclass{} class
alongside the default policy, which is \schednormal{}.\footnote{There is,
confusingly, also an Idle scheduling \textit{class}, but that is inaccessible to
userspace and exists solely to manage the core's transition in and out of being
actually idle (\ie{} running nothing).} 

Linux developers have improved \schedidle{} over the years, and it is meant to
support best effort workloads. It was also recently extended to have a
\cgroups{} interface file ~\cite{lkml-idle-cgroup}, so setting cpu.idle to
contain 1 puts the whole group under the \schedidle{} policy.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-idle-low.png}
        \caption{\schedidle{} in low load (85\%)}\label{fig:srv-bg-idle-low}
        \vspace{12pt}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[t]{\columnwidth}
        \includegraphics[width=\columnwidth]{graphs/srv-bg-idle-high.png}
        \caption{\schedidle{} in high load (95\%)}\label{fig:srv-bg-idle-high}
        \vspace{12pt}
    \end{subfigure}
    \vspace{4pt}
    \caption{\fk{XXX}}\label{fig:srv-bg-idle}
\end{figure}

To determine whether \schedidle{} solves the problem with \cgroups{}
weights, we run the same microbenchmark, using \schedidle{} for the
image resize job instead of using a low \cgroups{} weight;
\autoref{fig:srv-bg-idle} shows the results.  The \schedidle{} policy
for the \normalclass{} reduces the impact of the scheduling anomalies
but doesn't fix them.  In the appendix \autoref{s:alternatives} we also show that other
scheduling classes are not satisfactory alternatives to \cgroups{}.

\subsection{Summary}

From this section we conclude that Linux's \cgroups{} implementation
doesn't meet the specification stated in the documentation: each group
doesn't get time proportional to its weight as a share of the sum of
weights of runnable groups.  The reason Linux fails to achieve the
specification is because Linux uses per-CPU run-queues, and enforces
weights only per run-queue.  This implementation can result in
scheduling anomalies where one core doesn't realize that another core
has a runnable high-weight process, resulting in occasional latency
spikes for client requests to the high-weight process.





