%-------------------------------------------------------------------------------
\section{Introduction}
\label{s:intro}
%-------------------------------------------------------------------------------

Cloud providers like AWS and Google, as well as orchestration systems
like Kubernetes, allow users to run services by attaching to each an amount of
resources the service will get. This includes compute (a number of vCPUs),
memory, and sometimes network bandwidth~\cite{aws-ec2-resources,
kubernetes-resources}; the resource this paper focuses on is CPU. The guarantee
developers get is that the service will have undisturbed access to that amount
of resources.

Enforcing this guanatee is hard. If providers leave the CPUs idle so they are
available when the service needs them, that leads to a utilization problem. The
load on most applications run in the cloud is variable and unpredictable, so to
account for this developers choose the amount of resources to reserve based on
the expected peak load~\cite{borg, nu, overprovision}. This means that services
with reserved resources rarely use the full reservation they asked for. 

What most systems do instead is allow other workloads to run
opportunistically on temporarily unutilized resources. Such workloads
include \textit{best effort} ones, which have no reservations, as well
letting services with reservations temporarily \textit{burst}, meaning
use more CPUs than they asked for. Having these additional
Quality-Of-Sercie (QOS) classes solves the utilization problem:
\textit{guaranteed} services with reservations get access to the CPUs
they requested when desired, while best effort or bursting services
can make use of otherwise idle resources.  Kubernetes, for example,
provides exactly the above three service classes for
pods~\cite{kubernetes-pod-qos-types}, which are named BestEffort,
Guaranteed, and Burstable. AWS similarly supports both guaranteed- and
burstable-style reservations, with their M and T instance
types~\cite{aws-ec2-burstable,aws-ec2-resources}.

\paragraph{\cgroups{} weights.} To support these classes, many systems use Linux's
\cgroups{}~\cite{cgroups-kerneldocs} to enforce CPU reservations.  For
example, most modern containers rely on \cgroups{} for CPU isolation:
all Open Container Initiative (OCI) compliant containers, including
Kubernetes but also Docker, CRI-O, and containerd
do~\cite{oci-cgroups,docker-docs-cgroups,container-isolation-article}. VM
frameworks, including Firecracker, AFaas and libvirt, also rely on
\cgroups{} to manage CPU time
reservation~\cite{firecracker-cgroups,afaas,libvirt-cgroups}.

Weight is the part of the \cgroups{} interface these systems use to
separate applications in different QOS classes and enforce CPU
reservations.  The CPU time a group (the processes in one class) is
entitled to is the fraction it represents of total weight on the
machine. Kubernetes, for example, enforces reservations by mapping the
number of cores an application reserves to a weight: each core is
worth weight 1024, and an application's weight is the number of cores
it reserved * 1024. As long as the machine is not oversubscribed on
reservations (\ie{} the total weight on the machine is not greater
than its core count times 1024), the weight of the application should
entitle it to at least the number of cores it asked for. Applications
without any reservations share the resources of a single group that
has the smallest weight possible.

Weights are a desirable interface for several reasons. Unlike strict core
assignments, weights allow applications to burst beyond the exact number of
cores they asked for. This leads to higher CPU utilization: guaranteed servers
with bursting load, as well as best effort applications, can make use of unused
cores when load on reserved applications is low; this is a common case because
tenants provision for peak load. Weights also keep their relative values even
when the total weight in the system changes, and thus define bursting behavior:
if one application is not using the full number of cores its weight would
entitle it to, the remaining cores are divided up by weight, and thus a bursting
or a guaranteed server still has precedent over a best effort application. 

\paragraph{Problem.} The \cgroups{} documentation specifies that each group should get CPU
time proportional to its weight as a share of the sum of weights of
runnable groups~\cite{cgroups-kerneldocs}. But, we find that Linux
doesn't meet this specification.  For example, in the
\autoref{fig:kubernetes-unedited}, we run a best-effort application
with a weight of 1 and a guaranteed web application with 4 CPUs, which
Kubernetes assigns a weight of 157.  The latency increase we see after
starting the best effort tasks in \autoref{fig:kubernetes-unedited} is
much more than the 1\% CPU time the server should be losing out on
based on its weight.

As we show in more detail in \autoref{s:problem}, the root cause of
increases latencies is that the Linux schedule has a {\it per-core}
design: each core maintains its own run-queues and makes scheduling
decisions based on its local run queue.  This design scales well with
increasing number of cores, because no coordination with other cores
is required, and results in good core affinity: a
process runs frequently on the same core using with warm L1/L2 caches.
But, because each core makes its scheduling decisions independently,
one core may schedule a low-weight process, unaware that a high-weight
process is runnable and waiting on another. This results in a delay
for the heavy-weight process to run and causes increased latencies for
client requests.  The Linux load balancer periodically moves processes
between run-queues, but at a slower time scale than it makes
scheduling decisions and it balances load, not weight.

\paragraph{\sys.} A key challenge this paper addresses is
enforcing weights globally as the \cgroups{} documentation specifies,
while achieving good multi-core scalability and providing
core-affinity for processes.  To enforce weights globally, \sys has a
novel, {\it global} design: \sys assigns each runnable process a
virtual time based on its weight and maintains a heap of processes
sorted by their virtual times.  Each core consults the heap and runs
the process at the front of the heap. Since the heap is global and
sorted, a core will not miss a runnable high-weight process and thus
avoids the scheduling anomalies that Linux experiences.

To implement the global heap in scalable manner, \sys uses the relaxed
\textit{multiqueues} data structure~\cite{multiqueue:2015} for
scheduling processes. Multiqueues use a power-of-two
approach~\cite{mitzenmacher:power} to achieve scalability: it
maintains $c \times d$ queues (where $c$ is the number of cores and
$d$ a small constant, typically 2), inserts in a random queue, and
consults two queues to pick the lowest number. The power-of-two
approach guarantees that the ensures that the chosen number is close
to the global minimum (with high probability).  As described in detail
in \autoref{sec:global}, \sys augments multiqueues to take
core-affinity into account when selecting a process to run, so that
hot processes can benefit from warm L1/L2 caches.

\fk{summarize implementation and results}

The contributions of this paper are as
follows: 
\begin{enumerate}
    \item identifying as the reason for Linux scheduling anomalies
      that Linux fails to implement \cgroups{} correctly and doesn't
      enforce weights across cores;
    \item an algorithm for assigning a global virtual time to runnable
      processes based on their weights 
    \item using multiqueues for process scheduling based on their
      assigned virtual time and augmenting
      multiqueues to support core-affinity for processes
    \item \fk{an implementation}
    \item \fk{an eval}
\end{enumerate}
