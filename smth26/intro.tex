%-------------------------------------------------------------------------------
\section{Introduction}
\label{s:intro}
%-------------------------------------------------------------------------------

Cloud providers like AWS and Google, as well as orchestration systems
like Kubernetes, allow users to run services by attaching to each an amount of
resources the service will get. This includes compute (a number of vCPUs),
memory, and sometimes network bandwidth~\cite{aws-ec2-resources,
kubernetes-resources}; the resource this paper focuses on is CPU. The guarantee
developers get is that the service will have undisturbed access to that amount
of resources.

Enforcing this guanatee is hard. If providers leave the CPUs idle so they are
available when the service needs them, that leads to a utilization problem. The
load on most applications run in the cloud is variable and unpredictable, so to
account for this developers choose the amount of resources to reserve based on
the expected peak load~\cite{borg, nu, overprovision}. This means that services
with reserved resources rarely use the full reservation they asked for. 

What most systems do instead is allow other workloads to run
opportunistically on temporarily unutilized resources. Such workloads
include \textit{best effort} ones, which have no reservations, as well
letting services with reservations temporarily \textit{burst}, meaning
use more CPUs than they asked for. Having these additional
Quality-Of-Sercie (QOS) classes solves the utilization problem:
\textit{guaranteed} services with reservations get access to the CPUs
they requested when desired, while best effort or bursting services
can make use of otherwise idle resources.  Kubernetes, for example,
provides exactly the above three service classes for
pods~\cite{kubernetes-pod-qos-types}, which are named BestEffort,
Guaranteed, and Burstable. AWS similarly supports both guaranteed- and
burstable-style reservations, with their M and T instance
types~\cite{aws-ec2-burstable,aws-ec2-resources}.

To support these classes, many systems use Linux's
\cgroups{}~\cite{cgroups-kerneldocs} to enforce CPU reservations.  For
example, most modern containers rely on \cgroups{} for CPU isolation:
all Open Container Initiative (OCI) compliant containers, including
Kubernetes but also Docker, CRI-O, and containerd
do~\cite{oci-cgroups,docker-docs-cgroups,container-isolation-article}. VM
frameworks, including Firecracker, AFaas and libvirt, also rely on
\cgroups{} to manage CPU time
reservation~\cite{firecracker-cgroups,afaas,libvirt-cgroups}.

Weight is the part of the \cgroups{} interface these systems use for
enforcing the reservations guaranteed applications have, while still
allowing best-efforts applications without reservations to run
opportunistically.\footnote{Other operating systems expose a similar
interface, for instance Windows exposes a number of shares.}  For
example, Kubernetes creates one top level group for all best effort
services, called \texttt{kubepods-besteffort}, inside which all
best-effort
pods are placed, and assigns it the lowest possible weight of 1.
Kubernetes calculates the weight that guaranteed
pod gets based on the amount of milliCPUs it requests.

The \cgroups{} documentation specifies that each group should get CPU
time proportional to its weight as a share of the sum of weights of
runnable groups~\cite{cgroups-kerneldocs}. But, we find that Linux
doesn't meet this specification.  For example, in the
\autoref{fig:kubernetes-unedited}, we run a best-effort application
with a weight of 1 and a guaranteed web application with 4 CPUs, which
Kubernetes assigns a weight of 157.  The latency increase we see after
starting the best effort tasks in \autoref{fig:kubernetes-unedited} is
much more than the 1\% CPU time the server should be losing out on
based on its weight.

As we show in more detail in \autoref{s:problem}, the problem that
leads to the increased latencies observed is that Linux will run a
low-weight process on one core, unaware that a high-weight process is
runnable and waiting on another.  This happens because Linux uses
per-core run queues and each core searches its own run-queue when
making a scheduling decision and doesn't search other core's queues
for runnable heavy-weight processes (which doesn't scale with
increasing number of cores).  A key challenge this paper addresses is
enforcing global weights (as the \cgroups{} documentation specifies)
while achieving good multi-core scalability for scheduling operations.

\fk{explain \sys: global virtual time and multiqueue}

\fk{summarize implementation and results}

The contributions of this paper are thus as
follows: 
\begin{enumerate}
    \item identifying as the reason weights  are often violated is
    that \cgroups{} does a poor job of enforcing the weights across cores;
    \item \sys
\end{enumerate}
