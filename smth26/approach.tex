\section{Approach/Solution}\label{s:approach}

Creating a categorical split between LC and BE enforced by strict priority
scheduling solves these problems: it by definition ensures cpu isolation, it is
more easily enforced, and allows for more unfairness during high load.

\subsection{Priority scheduling ensures isolation}

By definition strict priority scheduling is global, and means: BE only ever runs
if there is no queued but runnable LC thread. Thus, when implemented correctly,
this solves the problem we saw that BE would sometimes run even while another
core had a queued LC thread.

\subsection{tick-based vs event-based}

Another benefit of priority-based scheduling between LC and BE rather than
fair-share based, is that the basis of the former is events, whereas the basis
of the latter is ticks. This is because, unless the set of runnable tasks
changes, if you were running the correct priority class before, you still will
be. This is not true for fair-share scheduling, where while it was running the
current thread might have gone over its share. The event-based separation of
categorical priorities limits the amount of synchronization points that the
scheduler needs to have in order to enforce a global priority.

\subsection{Unfairness in a distributed setting}

Much of scheduling has been centered around fairness, and being starvation-free.
We argue that the increased unfairness brought on by priority scheduling is not
only accetable, but desirable.

When the load is low, this is never a problem: even in strict priority system,
as long as the LC load does not use 100\% of the cputime on all the cores, the
BE tasks will get time to run, either while LC tasks are blocked, or because the
LC tasks don't use all the available cores.

It is only under high load, where all the cores are running LC tasks all the
time, that starvation is possible. However, in a distributed setting starvation
is good for a couple reasons.

The first reason is that, in a distributed setting, the machine scheduler is
only a small part of the picture. A distributed scheduler manages the load that
each machine receives, and monitors the state of all the machines. This means
that if a machine is experiencing a higher than usual load, this is expected to
be a temporary state while the distributed scheduler acts by migrating load or
the autoscaler starts new instances.

Another reason is that a machine experiencing high load is likely a locallized
problem. On a large time scale (hours), the overall load in a datacenter is
well-known. On the order of seconds and minutes, individual microservices' loads
may vary, but because of their statistical independence it is well-known that
broadly load will stay stable. 

Finally, we argue that the correct thing for a temporarily overloaded machine to
do is to just run the LC workloads as best it can, keeping the latencies low,
and wait for the distributed scheduler to do it's job. The whole reason that BE
tasks are BE is the fact that they are not latency sensitive, and are robust to
not making progress for large-but-bounded amounts of time.