\section{Approach}\label{s:approach}

We propose an extended \cgroups{} API where BE tasks are put in a separate class
\beclass{}, and a scheduler where the separation of \beclass{} from the default
LC class is enforced by strict priority scheduling.\footnote{here we do not mean
priorities in the way there are used in nice values, but rather a strict
priority where threads of a higher priority always run first.} As we show in
\autoref{ss:approach:solves-problems}, doing so solves the problems laid out in
\autoref{s:problem}. Strict priority scheduling runs the risk of starving the
lower class, which we avoid by separating critical services from application logic,
which we discuss in \autoref{ss:approach:parking}.

\subsection{\beclass{} solves the problems}\label{ss:approach:solves-problems}

\beclass{}, and the categorical split it create between LC and BE, solves the
problems identified in \autoref{s:problem}.

\textbf{\autoref{ss:problem:weights-local}} Strict priority scheduling solves
this problem because it is by definition global.

\textbf{\autoref{ss:problem:cross-core-hard}} If the scheduler is enforcing
priorities, then unless the set of runnable tasks changes, if the scheduler was
running the correct priority class before, it still will be. Thus the scheduler
only needs to synchronize across cores on \textit{class boundary crossings}, ie
when a core starts running a lower class process, or when a core enqueues a
higher class one. 

\textbf{\autoref{ss:problem:quantum}} Strict priorities being enforced at class
boundary crossings rather than ticks also addresses the final problem we saw. It
stops the fidelity of the isolation from being dependent on the length of the
tick, because the kernel is already involved in the events that cause the class
boundary crossings.

\subsection{Parking during high load}\label{ss:approach:parking}

The goal of parking is to maintain the strict priority isolation between LC and
BE under extremely high load, while preserving the current progress of the BE
application. We do this by separating BE processes' logic into application-level
and kernel-level; and ensure that during high load BE processes only get to run
on the kernel-level. This means that, in the parked state, a BE makes no
application-level progress, but remains runnable until the load has come
down.\hmng{should I break this down into subsections? I'm basically already
introducing the claims and then going through them step by step}

Because BEs are best effort, pausing application-level progress is not a
problem: coordinators and other processes they interact with expect them to make
slow progress or even be killed. For example, suppose a mapper has a heartbeat
with a coordinator, that includes the mapper's progress. In the parked state,
handling application-level heartbeats is paused until after the load goes down
and CPU time is available again to run the BE. If the mapper does not respond
for a while, without the connection dying, the coordinator can choose to kill
that instance and restart the job, or it can choose to wait. This leaves the
decision of how to handle the BE being stuck on a machine with high LC load up
to the application.

They key thing in order for the mapper to be able to resume after being parked
is that the connection between the mapper and manager not die. This is why
critical things like keeping TCP connections alive are handled on the kernel
level. The kernel is responsible for acknowledging the packages coming in, and
buffering the contents for the parked process. It does the same for other
intermediate events like timers firing and i/o completions.

Parking makes sense because a machine experiencing high load in a datacenter is
likely a localized problem, both in space (as in, will only effect some small
number of machines) and time (as in, the high load on the machine won't last
long). It is localized in space because microservices' loads are independent
from one another: on the order of seconds and minutes, individual microservices'
loads may vary, but broadly load is known to be predictable, so even if one
machine is experiencing high load other machines won't be~\cite{TODO}. It is
localized in time because of the distributed setting: a distributed scheduler
monitors the state of all the machines and migrates load when it sees high CPU
utilization, or an autoscaler starts new instances, shifting the excess LC load
to remote machines and thereby allowing BEs to run again on the local machine.