\section{Design/Parking?}\label{s:design}

We design a new scheduling class \beclass{} that sits at a lower priority than
\normalclass{}, which enforces latency critical applications' uncontended access
to the CPUs they reserved. Doing so addresses the two main concerns with Linux's
existing scheduling classes.

One is that the latency critical microservices stay in the same scheduling class
as they were before, which means existing mechanisms to balance CPU time between
different microservices remain viable.

To enforce reservations even under high load, without throttling the latency
critical services or killing the best effort ones, \beclass{} \textit{parks}
best effort processes.

\subsection{Parking}

As the load in \normalclass{} approaches 100\%, \beclass{} applications run less
frequently, since they only run in the times where \normalclass{} is not
utilizing the core. But even if \normalclass{} load is at 99\%, the \beclass{}
processes can still run for 1\% of the time, which is in principle more than the
0.01\% they would have been allowed to with the maximum weight split of 1/10000.
It is only when the load in \normalclass{} reaches 100\% CPU utilization for
extended periods of time that parking becomes different from running a process
with a small weight.

Parking addresses the potential issues with starvation by separating \beclass{}
processes' logic into application logic (user-space) and critical-state
management (kernel-space). When load spikes to 100\%, \beclass{} processes' run
only kernel-level services. We call this state \textit{parked}, because
although the application logic does not progress, the BE is not completely
starved because kernel-space handlers still run.

Our goal is to make parking as transparent as possible to the applications. On
the most immediate level, parking needs to avoid causing deadlock or dropped
connections. But it is also desirable that the BE and the services it talks to
should not need to do special-cased error handling if the BE was parked. For
instance, suppose a MapReduce mapper has a heartbeat with a coordinator to check
in on the mapper's progress. What should happen is that handling
application-level heartbeats is paused until the load goes down, but the TCP
connection is kept alive and the mapper can respond to heartbeats and complete
its job when the load goes down. The coordinator would have in the meantime
likely marked the parked mapper as a straggler and started a backup job; if the
original temporarily parked mapper still finishes first it would simply send its
results via the existing TCP connection.

Creating the divide between application and critical operating system services
achieves this goal for us. To avoid deadlock, Linux already marks kernel-space
operations that use locks as non-preemptible, and ensures that non-preemptible
code is not interrupted. This marking means that it is not possible for a BE to
still hold a kernel-space lock after being descheduled. User-space locks do not
have such protection, but user-space locks are not shared across LC and BE
applications.

Linux also already manages I/O interrupts, including acknowledging network
packets, in kernelspace interrupt handlers. These run irrespective of whether
and when the userspace process is scheduled, and simply write the relevant
message buffers to the processes' memory. Thus, when the load goes down and the
parked process can run again, the read will return all the intermediate messages
at once. 

Timers are similar: if a BE processes' timer fires while the BE is parked, the
kernel will handle the initial interrupt and send the process a signal. Once the
load goes down and the BE runs, it will see the timer has fired.

Parking is expected to be short-lived, unlike approaches such as
checkpoint-restore. A machine in a datacenter experiencing that high of a load
is likely a localized problem, both in space (as in, will only effect some small
number of machines) and time (as in, the high load on the machine won't last
long). It is localized in space because microservices' loads are independent
from one another: on the order of seconds and minutes, individual microservices'
loads may vary, but broadly load is known to be predictable, so even if one
machine is experiencing high load other machines won't be~\cite{TODO}. It is
localized in time because of the distributed setting: a distributed scheduler
monitors the state of all the machines and migrates load when it sees high CPU
utilization, or an autoscaler starts new instances, shifting the excess LC load
to remote machines and thereby allowing BEs to run again on the local machine.