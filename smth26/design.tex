\section{Design}\label{s:design}

We design a new cgroup interface file, cpuset.min, that allows groups, \ie{}
applications, to have guaranteed access to their reserved CPUs, while at the
same time supporting bursting on cores that are reserved but not currently in
use. Best effort tasks need to be allowed to share cores with reserved
workloads.

\subsection{The Guarantees we want}

We imagine cpuset.min as a new interface file in the same cgroup controller as
the existing cpuset mechanisms. cpuset.min takes in a list of cpu numbers, in
the same way that existing cpuset mechanisms do.

Groups with min cores are guaranteed to always be able to run on those cores,
and to do so without interruption\hmng{should I asterisk this b/c not if the min
core is shared}. Concretely, the invariant we need to maintain is that if a
group has $m$ cpus that are designated as its min, and $t$ runnable threads,
then if $t \leq m$ each thread will have its own core to run on. If $t > m$,
then the $t$ threads will run on \textit{at least} $m$ cores. We make no
guarantees about bursting: if there are unused cpu cores, we do not enforce that
the extra cpu time will be allocated to align with any guarantees.

Developers can specify fractional CPUs by having the same cpu show up in
multiple groups' min cores lists. In that case, each overlapped cpu is shared
according to the weight specified in cpu.weight. We expect fractional cores will
only be used for reservations that ask for less than one core, and make no
guarantees about which min core a thread will run on. For example, if a core
were to reserve two cores, one of which was shared and the other was not, we do
not guarantee that the first thread to run will use the core that this group
alone has reserved.

Best effort workloads have no min cores: they are not guaranteed any CPU time,
and must not get CPU time on a given core $c$ if there is a runnable thread that
has that $c$ as a min core.

\subsection{The design that gets us those guarantees}

We define \textit{events}, which are the relevant points in time at which
cross-core scheduling decisions need to take place, in order to maintain the
desired invariants. They are related to \textit{enqueue}, which is when a thread
wakes up and needs to be enqueued on some cores runqueue, and \textit{dequeue},
which is when a previously runnable thread exits or blocks and is removed from
that cores runqueue.

There are two cases in which \textit{enqueue} requires interposition. Let $t$ is
again the number of threads and $m$ the number of reserved cores, the two cases
are: 1. if $t < m$ for waking group, and 2. if $t \geq m$ for the waking group.
In the first case, that means that at least one min core is currently not being
used by the group that reserved them, and the new thread needs to start running
there. This may mean moving currently bursting load off that core. If $t \geq
m$, the new thread should join the core already running that group with the
lowest load.

\textit{Dequeue} can similarly be broken down into cases: 1. if the thread
exiting is the last thread in that group on that core, 2. if it is not. If it is
the last thread, then the core needs to try to steal another runnable but queued
thread from that group on a different core. If that's not possible, then any
other group can burst: if there is queued best effort work run that, otherwise
steal a thread from the most loaded core. If the thread being dequeued is not
the last of its group on the core, no further action is required.

These interventions in \textit{enqueue} and \textit{dequeue} enforce the desired
invariants.




