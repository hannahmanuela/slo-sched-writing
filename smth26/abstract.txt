Many providers use to separate the applications of tenants in different CPU classes: for example guaranteed for applications with CPU reservations (such as servers) and best-effort (for applications with no CPU reservations such as back-ground tasks). They offer the lower-class at lower price than the higher class. This approach allows the provider to obtain high CPU utilization, because they can pack best-effort applications with guaranteed applications when the guaranteed applications isn’t using its full reservation, which is often the case because tenants provision for peak load.

This paper finds that Linux’s implementation of doesn’t meet its specification, and as a result guaranteed servers can experience high tail latencies occasionally, because they are not being scheduled, even though they are runnable. The core reason for such scheduling anomalies is that Linux’s scheduler has a decentralized design to be efficient on today’s machines with large number of core, and doesn’t search other cores’ runqueues for runnable high-class processes when making a scheduling decision, relying on a load balancer to once in a while shuffle processes between runqueues.

This paper introduces GlobalHeap a new centralized scheduler that implements the specification correctly in a scalable manner. It maintains a global virtual time to order all runnable processes by their CPU weight, and uses the approximate multiqueues data structure  to implement a global heap of processes sorted by virtual time. GlobalHeap augments multiqueues to allow it to take core-affinity into account when selecting a process to run, so that the selected process can benefit from warm L1/L2 caches.

Experimental results with an implementation of GlobalHeap in Linux on a 56-core machines shows that GlobalHeap correctly implements the specification, avoids the scheduling anomalies that Linux suffers from, and consistently provides low tail-latencies for guaranteed servers.
