\section{Design/Parking?}\label{s:design}

We design a new scheduling class \beclass{} that sits below \normalclass{}, that
enforces LC's uncontended access to the CPUs it reserved. 

\beclass{} continues to enforce LC reservations even under high load, without
killing \beclass{} processes or throttling the LC workload, by introducing
\textit{parking}.

\subsection{Parking in Linux}

As LC load approaches 100\%, BE applications run proportionally less, since they
only run in the times where LC is not utilizing the core. But even if LC load
is at 99\%, the BEs can still run for 1\% of the time, which is in principle more
than the 0.01\% they would have been allowed to with a weight of 1/10000. It is
only when LC load reaches 100\% CPU utilization for extended periods of time
that parking becomes different from running a process with a small weight.

We address the potential issues with starvation by separating BE processes'
logic into application logic (user-space) and critical-state management
(kernel-space). When LC load spikes, BEs run only on the kernel-space level. We
call this state \textit{parked}, because although the application logic does not
progress, the BE is not completely starved because kernel-space handlers still
run.

Our goal is to make parking as transparent as possible to the applications. On
the most immediate level, parking needs to avoid causing deadlock or dropped
connections. But it is also desirable that the BE and the services it talks to
should not need to do special-cased error handling if the BE was parked. For
instance, a mapper that has a heartbeat with a coordinator to check in on the
mapper's progress, should not have to check on every read and write to the
connection with the coordinator to see if the connection timed out because it
was parked. What should happen is that handling application-level heartbeats is
paused until the load goes down, but the TCP connection is kept alive. 

In the event that the mapper becomes parked, the coordinator would see no
heartbeats, and correctly know that the mapper's progress is very slow/that it
is unable to run. The coordinator, with its knowledge of the size of the job and
the other mappers progress, can then choose to kill that mapper and restart the
job, or it can choose to wait.


Creating the divide between user-space and kernel-space achieves this goal for
us. To avoid deadlock, Linux already marks kernel-space operations that use
locks as non-preemptible, and ensures that non-preemptible code is not
interrupted. This means that it is not possible for a BE to still hold a
kernel-space lock after being descheduled. User-space locks do not have such
protection, but user-space locks are not shared across LC and BE applications.
Linux also already manages I/O interrupts, including acknowledging network
packets, in kernelspace interrupt handlers. These run irrespective of whether
and when the userspace process is scheduled, and simply write the relevant
message buffers to the processes' memory. This means that when the load goes
down and the parked process can run again, the read will return all the
intermediate messages at once. Timers are similar: if a BE processes' timer
fires while the BE is parked, the kernel will handle the initial interrupt and
send the process a signal. Once the load goes down and the BE runs, it will see
the timer has fired.

Parking is also expected to be short-lived, which positions it more as a
temporary disaster-refief mechanism than as a long-term state\hmng{clumsy/not
saying anything}. A machine in a datacenter experiencing that high of a load is
likely a localized problem, both in space (as in, will only effect some small
number of machines) and time (as in, the high load on the machine won't last
long). It is localized in space because microservices' loads are independent
from one another: on the order of seconds and minutes, individual microservices'
loads may vary, but broadly load is known to be predictable, so even if one
machine is experiencing high load other machines won't be~\cite{TODO}. It is
localized in time because of the distributed setting: a distributed scheduler
monitors the state of all the machines and migrates load when it sees high CPU
utilization, or an autoscaler starts new instances, shifting the excess LC load
to remote machines and thereby allowing BEs to run again on the local machine.


\section{Implementing \beclass{} in Linux}\label{s:implementation}

In order to enforce LC's uncontended access to CPUs, the goal of the
implementation is to enforce the maxim that no \beclass{} userspace process is
ever running on cores reserved for a \normalclass{} workload if a \normalclass{}
process is runnable and queued.\hmng{what about fractional core reservations?
maybe this fits into question of what are weights good for} In order to do so,
the scheduler must enforce the priorities in three different places:
\begin{enumerate}
    \item \local: in picking the next process to run on each core, it must ensure
        that no \beclass{} process will be chosen if there is a runnable
        \normalclass{} process,
    \item \entry: when waking up a \normalclass{} process on a core already
        running a \normalclass{} process, it must look for other cores running
        \beclass{} entities to go interrupt,
    \item \exit: when the last \normalclass{} process on a core's queue blocks
        or exits, the core must try to steal queued \normalclass{} processes from
        other cores before running a \beclass{} process.
\end{enumerate}

In order to implement \beclass{} in Linux, we build on \schedidle{}, an existing
scheduling \textit{policy} in Linux, rather than creating an actual new
scheduling class. This is because \schedidle{} already has some of the features
we want for \beclass{}.

\subsection{\schedidle{} lends itself well to \beclass{}}

Scheduling \textit{classes} in Linux can have multiple \textit{policies}, and
\schedidle{} lives within the \normalclass{} class alongside the default policy
of the \normalclass{} class, which is \schednormal{}.\footnote{There is, very
confusingly, also an Idle scheduling \textit{class}, but that not accessible to
userspace and exists solely to manage the core's transition in and out of being
actually idle (ie running nothing).} The existing \schedidle{} policy is in many
ways not different from a low weight \schednormal{} process: both are kept on
the same runqueues as all the other \schednormal{} processes, and \schedidle{}
just has a predefined low weight of 3~\cite{weight-idleprio}.

One way that \schedidle{} is promising as a foundation for our implementation of
\beclass{} is that \schedidle{} was extended to be accessible via the \cgroups{}
API recently~\cite{lkml-idle-cgroup}: a whole groups' policy can be set to
\schedidle{} via the \cgroups{} interface. This means that building on
\schedidle{} allows us to get for free the ability to use the \cgroups{} API to
mark groups as BE. We thus have two ways to mark things as BE: we can mark
individual processes by setting their policy to \schedidle{}, or we can do so
for a whole group via the \cgroups{} API.

An additional benefit to using \schedidle{} as the starting point for our
implementation is that, after a push by Facebook, Linux developers already added
what is in effect the \entry{} check from the \beclass{}
design~\cite{fixing-idle-article}. In 2019, Linux added a check when a
\schednormal{} process becomes newly runnable on a core already running something
in \schednormal{}. This new check looks for other cores that might be currently
running a \schedidle{} process, and migrates the new process there.

\subsection{Implementating \beclass{} in \schedbe{}}

To achieve priority scheduling on top of \schedidle{}, our implementation adds
the \local{} and \exit{} parts of the \beclass{} design. We call the resulting
policy \schedbe{}. While it is technically still a policy, it implements the
desired behavior of \beclass{}, and as a result behaves as if it was a
scheduling class.

To enforce the \local{} part of the design, which calls for the local (\ie{}
single runqueue) isolation of \schedbe{} processes, we ensure that the task
chosen to run from the runqueue is only \schedbe{} if everything else on the
runqueue is as well (\autoref{ss:implementation:local}). To enforce the \exit{}
check, we add a cross-core check and potentially steal work, which completes the
global policy enforcement (\autoref{ss:implementation:exit}).

\subsubsection{Enforcing the local policy}\label{ss:implementation:local}

In order to enforce that ruqueues only run \schedbe{} threads when there are no
runnable \schednormal{} ones, our implementation interferes in two places. 

Because in existing Linux \schedidle{} and \schednormal{} share a runqueue, so
will the new \schedbe{} and \schednormal{}, since doing so maintains a
complicated existing infrastructure around the runqueue for things like
accounting and load balancing. This means that the function choosing the next
task from the runqeueue will be potentially looking at a mix of both policies.
We add an initial check that establishes whether there are any \schednormal{}
threads on the runqueue, and skips all \schedbe{} ones if that is the case. 

The second change is necessary because the first breaks an existing eligibility
mechanism. In order to help maintain fairness, Linux currently accounts for the
difference between the fair share processes should have gotten and the time they
actually got, and stores that `lag'. Processes that have gotten more time than
they should (ie have negative lag), are marked as ineligible and not considered
when choosing what to run next. Since \schedbe{} threads are now potentially not
being run for a long time, there is a potential for deadlock: a \schednormal{}
task has been running for a while and accrued enough time that its lag is
negative and it is ineligible. However, if the only other thread is \schedbe{}
then we won't run that either because there is a runnable \schednormal{} task on
the runqueue. In order to avoid this situation, the implementation removes the
eligibility criterion in choosing what to run next.\hmng{why is that ok?}


\subsubsection{Enforcing the global policy}\label{ss:implementation:exit}

Ensuring the \entry{} and \exit{} checks requires interposing on Linux's wakeup
and exit codepaths. Linux already special-cases on the wakeup path, although
only checks if the thread itself is marked as idle, and not if the group as a
whole is, and for \schedbe{} we check both.

\schedbe{} adds a check on the exit path: if the thread chosen to run next is
\schedbe{}, but the previous one was \schednormal{}, then our implementation
tries to steal a queued \schednormal{} task from a different core. Specifically,
it picks the core with the max number of queued but runnable \schednormal{}
threads. It steals only one, in order to not overzealously steal. This choice
mirrors what Linux does when a core would otherwise go completely idle.