
\section{Why}
\label{sec:why}

In order to understand why this is happening, we examine a trace of the
experiment. We use schedviz~\cite{schedviz}, a tool that visualizes perf traces,
to examine the scheduling decisions Linux is making.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{graphs/schedviz.png}
        \caption{Each thread is a different color. Circles represent which
    thread is running on that core, while rectangles underneath show waiting but
    runnable threads. Core numbers are 2,4,6,8 because this was run on a NUMA
    node so those are the cores that are closest to each other
    }\label{fig:schedviz}
\end{figure*}

In figure~\ref{fig:schedviz}, we look at a 10ms outtake from the run whose
results are in figure~\ref{fig:unedited-weight-low-two}, that shows the problem
occuring. The undesirable behavior we see here is that, on core 6, the red
process that is running the whole time is a background process. All the active
server threads, shown in varying shades of blue, are running and, more
importantly, waiting on the other cores.~\hmng{wait so does that mean if it was
always a new/running-low-weight core that picked up the new request then this
problem would go away? I guess that's what running only one thread per core
would do \ldots / that's what I'm ensuring with my patch? it just seems like by
having this many workers we are exacerbating the problem? }

The reason this happens is that Linux maintains a separate runqueue on each
core. This avoids the synchronization overheads of maintaining a global
runqueue, but also means that the weight is ony strictly enforced within the
individual runqueues, ie within each core. This leads to the depicted failure
mode, where an LC task is waiting on one core while another runs a BE task.
